{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "# !pip install matplotlib numpy pandas tqdm nltk\n",
    "\n",
    "# for separating ingredients vs non-ingredients\n",
    "# NOTE: if using Windows to run this, need to download GNU Wget\n",
    "# !wget -c https://raw.githubusercontent.com/williamLyh/RecipeWithPlans/main/ingredient_set.json -O ingredient_set.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, MultiStepLR, CosineAnnealingWarmRestarts\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from nltk.translate import meteor\n",
    "\n",
    "from data import *\n",
    "from encoder_decoder import *\n",
    "from train import *\n",
    "from eval import *\n",
    "from utils import *\n",
    "\n",
    "# required for bleu\n",
    "# nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 31989101\n",
    "HIDDEN_SIZE = 256\n",
    "MAX_INGR_LEN = 150 # fixed from assignment\n",
    "MAX_RECIPE_LEN = 600\n",
    "DROPOUT = 0.1\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## ensuring reproducibility\n",
    "def reset_rng():\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "reset_rng()\n",
    "\n",
    "# to easily read ingredients and instructions\n",
    "pd.set_option('display.max_colwidth', 2000)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./Cooking_Dataset\"\n",
    "add_intermediate_tag=False\n",
    "\n",
    "train_df_orig = pd.read_csv(os.path.join(data_root, \"train.csv\"), usecols=['Ingredients', 'Recipe'])\n",
    "dev_df_orig = pd.read_csv(os.path.join(data_root, \"dev.csv\"), usecols=['Ingredients', 'Recipe'])\n",
    "test_df_orig = pd.read_csv(os.path.join(data_root, \"test.csv\"), usecols=['Ingredients', 'Recipe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples before preprocessing: 101340\n",
      "Number of data samples after preprocessing: 100637 (99.306%)\n"
     ]
    }
   ],
   "source": [
    "train_df = preprocess_data(train_df_orig, max_ingr_len=MAX_INGR_LEN, max_recipe_len=MAX_RECIPE_LEN, add_intermediate_tag=add_intermediate_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples before preprocessing: 797\n",
      "Number of data samples after preprocessing: 793 (99.498%)\n"
     ]
    }
   ],
   "source": [
    "dev_df = preprocess_data(dev_df_orig, max_ingr_len=MAX_INGR_LEN, max_recipe_len=MAX_RECIPE_LEN, add_intermediate_tag=add_intermediate_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples before preprocessing: 778\n",
      "Number of data samples after preprocessing: 774 (99.486%)\n"
     ]
    }
   ],
   "source": [
    "test_df = preprocess_data(test_df_orig, max_ingr_len=MAX_INGR_LEN, max_recipe_len=MAX_RECIPE_LEN, add_intermediate_tag=add_intermediate_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100637 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100637/100637 [00:06<00:00, 15556.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44315"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocabulary(add_intermediate_tag=add_intermediate_tag)\n",
    "vocab.populate(train_df)\n",
    "vocab.n_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RecipeDataset(train_df, vocab)\n",
    "# subset_train_ds = RecipeDataset(train_df[:250], vocab) # ! REMOVE LATER\n",
    "dev_ds_val_loss = RecipeDataset(dev_df, vocab, train=True) # used for getting validation loss\n",
    "dev_ds_val_met = RecipeDataset(dev_df, vocab, train=False) # used for getting validation BLEU, and other metrics\n",
    "test_ds = RecipeDataset(test_df, vocab, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder (Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=300\n",
    "encoder = EncoderRNN(vocab.n_unique_words, embedding_size=embedding_size, hidden_size=HIDDEN_SIZE, padding_value=vocab.word2index(PAD_WORD)).to(DEVICE)\n",
    "# in the training script, decoder is always fed a non-end token and thus never needs to generate padding\n",
    "# also it should never generate \"<UNKNOWN>\"\n",
    "decoder = DecoderRNN(embedding_size=embedding_size,hidden_size=HIDDEN_SIZE, output_size=vocab.n_unique_words-1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/30, enc lr scheduler: [0.001], dec lr scheduler: [0.001]\n",
      "(Epoch 0, iter 10/787) Average loss so far: 10.210\n",
      "(Epoch 0, iter 20/787) Average loss so far: 6.945\n",
      "(Epoch 0, iter 30/787) Average loss so far: 6.156\n",
      "(Epoch 0, iter 40/787) Average loss so far: 6.180\n",
      "(Epoch 0, iter 50/787) Average loss so far: 6.071\n",
      "(Epoch 0, iter 60/787) Average loss so far: 6.040\n",
      "(Epoch 0, iter 70/787) Average loss so far: 6.024\n",
      "(Epoch 0, iter 80/787) Average loss so far: 5.991\n",
      "(Epoch 0, iter 90/787) Average loss so far: 5.968\n",
      "(Epoch 0, iter 100/787) Average loss so far: 5.922\n",
      "(Epoch 0, iter 110/787) Average loss so far: 5.875\n",
      "(Epoch 0, iter 120/787) Average loss so far: 5.845\n",
      "(Epoch 0, iter 130/787) Average loss so far: 5.806\n",
      "(Epoch 0, iter 140/787) Average loss so far: 5.735\n",
      "(Epoch 0, iter 150/787) Average loss so far: 5.680\n",
      "(Epoch 0, iter 160/787) Average loss so far: 5.623\n",
      "(Epoch 0, iter 170/787) Average loss so far: 5.582\n",
      "(Epoch 0, iter 180/787) Average loss so far: 5.544\n",
      "(Epoch 0, iter 190/787) Average loss so far: 5.465\n",
      "(Epoch 0, iter 200/787) Average loss so far: 5.394\n",
      "(Epoch 0, iter 210/787) Average loss so far: 5.393\n",
      "(Epoch 0, iter 220/787) Average loss so far: 5.290\n",
      "(Epoch 0, iter 230/787) Average loss so far: 5.281\n",
      "(Epoch 0, iter 240/787) Average loss so far: 5.231\n",
      "(Epoch 0, iter 250/787) Average loss so far: 5.168\n",
      "(Epoch 0, iter 260/787) Average loss so far: 5.150\n",
      "(Epoch 0, iter 270/787) Average loss so far: 5.102\n",
      "(Epoch 0, iter 280/787) Average loss so far: 5.049\n",
      "(Epoch 0, iter 290/787) Average loss so far: 5.066\n",
      "(Epoch 0, iter 300/787) Average loss so far: 5.038\n",
      "(Epoch 0, iter 310/787) Average loss so far: 4.920\n",
      "(Epoch 0, iter 320/787) Average loss so far: 4.976\n",
      "(Epoch 0, iter 330/787) Average loss so far: 4.909\n",
      "(Epoch 0, iter 340/787) Average loss so far: 4.922\n",
      "(Epoch 0, iter 350/787) Average loss so far: 4.880\n",
      "(Epoch 0, iter 360/787) Average loss so far: 4.831\n",
      "(Epoch 0, iter 370/787) Average loss so far: 4.802\n",
      "(Epoch 0, iter 380/787) Average loss so far: 4.801\n",
      "(Epoch 0, iter 390/787) Average loss so far: 4.774\n",
      "(Epoch 0, iter 400/787) Average loss so far: 4.762\n",
      "(Epoch 0, iter 410/787) Average loss so far: 4.732\n",
      "(Epoch 0, iter 420/787) Average loss so far: 4.735\n",
      "(Epoch 0, iter 430/787) Average loss so far: 4.684\n",
      "(Epoch 0, iter 440/787) Average loss so far: 4.666\n",
      "(Epoch 0, iter 450/787) Average loss so far: 4.636\n",
      "(Epoch 0, iter 460/787) Average loss so far: 4.611\n",
      "(Epoch 0, iter 470/787) Average loss so far: 4.631\n",
      "(Epoch 0, iter 480/787) Average loss so far: 4.562\n",
      "(Epoch 0, iter 490/787) Average loss so far: 4.581\n",
      "(Epoch 0, iter 500/787) Average loss so far: 4.557\n",
      "(Epoch 0, iter 510/787) Average loss so far: 4.531\n",
      "(Epoch 0, iter 520/787) Average loss so far: 4.605\n",
      "(Epoch 0, iter 530/787) Average loss so far: 4.482\n",
      "(Epoch 0, iter 540/787) Average loss so far: 4.493\n",
      "(Epoch 0, iter 550/787) Average loss so far: 4.508\n",
      "(Epoch 0, iter 560/787) Average loss so far: 4.512\n",
      "(Epoch 0, iter 570/787) Average loss so far: 4.454\n",
      "(Epoch 0, iter 580/787) Average loss so far: 4.416\n",
      "(Epoch 0, iter 590/787) Average loss so far: 4.421\n",
      "(Epoch 0, iter 600/787) Average loss so far: 4.424\n",
      "(Epoch 0, iter 610/787) Average loss so far: 4.410\n",
      "(Epoch 0, iter 620/787) Average loss so far: 4.393\n",
      "(Epoch 0, iter 630/787) Average loss so far: 4.366\n",
      "(Epoch 0, iter 640/787) Average loss so far: 4.340\n",
      "(Epoch 0, iter 650/787) Average loss so far: 4.346\n",
      "(Epoch 0, iter 660/787) Average loss so far: 4.316\n",
      "(Epoch 0, iter 670/787) Average loss so far: 4.296\n",
      "(Epoch 0, iter 680/787) Average loss so far: 4.361\n",
      "(Epoch 0, iter 690/787) Average loss so far: 4.279\n",
      "(Epoch 0, iter 700/787) Average loss so far: 4.280\n",
      "(Epoch 0, iter 710/787) Average loss so far: 4.271\n",
      "(Epoch 0, iter 720/787) Average loss so far: 4.278\n",
      "(Epoch 0, iter 730/787) Average loss so far: 4.239\n",
      "(Epoch 0, iter 740/787) Average loss so far: 4.287\n",
      "(Epoch 0, iter 750/787) Average loss so far: 4.258\n",
      "(Epoch 0, iter 760/787) Average loss so far: 4.218\n",
      "(Epoch 0, iter 770/787) Average loss so far: 4.239\n",
      "(Epoch 0, iter 780/787) Average loss so far: 4.239\n",
      "Average epoch loss: 5.006\n",
      "This epoch took 23.566594620545704 mins. Time remaining: 11.0 hrs 23.0 mins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:03,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 4.239502702440534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 28/199 [00:15<01:36,  1.77it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# enc_scheduler = MultiStepLR(encoder_optimizer, milestones=[15], gamma=0.1)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# dec_scheduler = MultiStepLR(decoder_optimizer, milestones=[15], gamma=0.1)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m identifier\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam_without_intermediate_tags_wd0_lr1e-3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 12\u001b[0m epoch_losses, log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbasic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                     \u001b[49m\u001b[43menc_lr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_lr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdev_ds_val_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdev_ds_val_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_ds_val_met\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_ds_val_met\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midentifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverbose_iter_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m save_log(identifier, log, encoder_optimizer, decoder_optimizer, enc_scheduler, dec_scheduler)\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/train.py:209\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, dataset, n_epochs, vocab, decoder_mode, batch_size, enc_lr_scheduler, dec_lr_scheduler, dev_ds_val_loss, dev_ds_val_met, identifier, min_bleu_to_save, verbose, verbose_iter_interval)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mprint\u001b[39m(msg)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m## get validation metrics\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m all_decoder_outs, all_gt_recipes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_ds_val_met\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmax_recipe_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_RECIPE_LEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m bleu \u001b[38;5;241m=\u001b[39m calc_bleu(all_gt_recipes, all_decoder_outs)\n\u001b[1;32m    212\u001b[0m meteor \u001b[38;5;241m=\u001b[39m calc_meteor(all_gt_recipes, all_decoder_outs, split_gt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/eval.py:229\u001b[0m, in \u001b[0;36meval\u001b[0;34m(encoder, decoder, dataset, vocab, batch_size, max_recipe_len, decoder_mode)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ingredients, recipes, ing_lens, _ \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;66;03m# ingredients: Tensor[N, L_i] padded ingredients\u001b[39;00m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;66;03m# recipes (List[List[str]]): list of len N, each element is a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;66;03m#                               list of len `gen_size`, which is the size of the generated sequence, and each element is a \u001b[39;00m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;66;03m#                                   str representing a single word in the generated recipe\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m         dec_outs \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mingredients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ming_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmax_recipe_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_recipe_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m         all_decoder_outs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dec_outs\n\u001b[1;32m    233\u001b[0m         all_gt_recipes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m recipes\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/eval.py:205\u001b[0m, in \u001b[0;36mget_predictions_iter\u001b[0;34m(ingredients, ing_lens, encoder, decoder, vocab, max_recipe_len, decoder_mode)\u001b[0m\n\u001b[1;32m    201\u001b[0m decoder_cell \u001b[38;5;241m=\u001b[39m enc_c_final\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# List[List[str]]\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m all_decoder_outs \u001b[38;5;241m=\u001b[39m \u001b[43meval_decoder_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_cell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mingredients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_recipe_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_decoder_outs\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/eval.py:144\u001b[0m, in \u001b[0;36meval_decoder_iter\u001b[0;34m(decoder, decoder_hidden, decoder_cell, encoder_houts, ingredients, max_recipe_len, vocab, decoder_mode)\u001b[0m\n\u001b[1;32m    141\u001b[0m decoder_topk_preds \u001b[38;5;241m=\u001b[39m decoder_out\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [N_valid]\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m## store generated output\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec_idx, valid_n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(decoder_topk_preds)), \u001b[43mvalid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    145\u001b[0m     valid_idx \u001b[38;5;241m=\u001b[39m valid_n\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    146\u001b[0m     all_decoder_outs[valid_idx]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    147\u001b[0m         vocab\u001b[38;5;241m.\u001b[39mindex2word[decoder_topk_preds[dec_idx]\u001b[38;5;241m.\u001b[39mitem()]) \u001b[38;5;66;03m# str\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_lr=1e-3\n",
    "min_lr = 1e-5\n",
    "n_epochs = 30\n",
    "batch_size=128\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=initial_lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=initial_lr)\n",
    "enc_scheduler = CosineAnnealingLR(encoder_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "dec_scheduler = CosineAnnealingLR(decoder_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "# enc_scheduler = MultiStepLR(encoder_optimizer, milestones=[15], gamma=0.1)\n",
    "# dec_scheduler = MultiStepLR(decoder_optimizer, milestones=[15], gamma=0.1)\n",
    "identifier=\"adam_without_intermediate_tags_wd0_lr1e-3\"\n",
    "epoch_losses, val_epoch_losses, log = train(encoder, decoder, encoder_optimizer, decoder_optimizer, train_ds, \n",
    "                     n_epochs=n_epochs, vocab=vocab, decoder_mode=\"basic\", batch_size=batch_size, \n",
    "                     enc_lr_scheduler=enc_scheduler, dec_lr_scheduler=dec_scheduler, \n",
    "                     dev_ds_val_loss = dev_ds_val_loss, dev_ds_val_met=dev_ds_val_met, identifier=identifier,\n",
    "                     verbose_iter_interval=10)\n",
    "\n",
    "save_log(identifier, log, encoder_optimizer, decoder_optimizer, enc_scheduler, dec_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(encoder, decoder, \"adam_without_intermediate_tags_wd0_lr1e-3_last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder (Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=300\n",
    "encoder_attn = EncoderRNN(vocab.n_unique_words, embedding_size=embedding_size, hidden_size=HIDDEN_SIZE, padding_value=vocab.word2index(PAD_WORD)).to(DEVICE)\n",
    "# in the training script, decoder is always fed a non-end token and thus never needs to generate padding\n",
    "# also it should never generate \"<UNKNOWN>\"\n",
    "# decoder = DecoderRNN(embedding_size=embedding_size,hidden_size=HIDDEN_SIZE, output_size=vocab.n_unique_words-2).to(DEVICE)\n",
    "decoder_attn = AttnDecoderRNN(embedding_size, hidden_size=HIDDEN_SIZE, output_size=vocab.n_unique_words-1, padding_val=vocab.word2index(PAD_WORD), dropout=DROPOUT).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/30, enc lr scheduler: [0.0001], dec lr scheduler: [0.0001]\n",
      "(Epoch 0, iter 10/787) Average loss so far: 10.670\n",
      "(Epoch 0, iter 20/787) Average loss so far: 10.593\n",
      "(Epoch 0, iter 30/787) Average loss so far: 10.411\n",
      "(Epoch 0, iter 40/787) Average loss so far: 9.926\n",
      "(Epoch 0, iter 50/787) Average loss so far: 9.157\n",
      "(Epoch 0, iter 60/787) Average loss so far: 8.323\n",
      "(Epoch 0, iter 70/787) Average loss so far: 7.634\n",
      "(Epoch 0, iter 80/787) Average loss so far: 7.154\n",
      "(Epoch 0, iter 90/787) Average loss so far: 6.809\n",
      "(Epoch 0, iter 100/787) Average loss so far: 6.570\n",
      "(Epoch 0, iter 110/787) Average loss so far: 6.415\n",
      "(Epoch 0, iter 120/787) Average loss so far: 6.326\n",
      "(Epoch 0, iter 130/787) Average loss so far: 6.235\n",
      "(Epoch 0, iter 140/787) Average loss so far: 6.208\n",
      "(Epoch 0, iter 150/787) Average loss so far: 6.159\n",
      "(Epoch 0, iter 160/787) Average loss so far: 6.150\n",
      "(Epoch 0, iter 170/787) Average loss so far: 6.124\n",
      "(Epoch 0, iter 180/787) Average loss so far: 6.119\n",
      "(Epoch 0, iter 190/787) Average loss so far: 6.113\n",
      "(Epoch 0, iter 200/787) Average loss so far: 6.093\n",
      "(Epoch 0, iter 210/787) Average loss so far: 6.094\n",
      "(Epoch 0, iter 220/787) Average loss so far: 6.086\n",
      "(Epoch 0, iter 230/787) Average loss so far: 6.098\n",
      "(Epoch 0, iter 240/787) Average loss so far: 6.042\n",
      "(Epoch 0, iter 250/787) Average loss so far: 6.090\n",
      "(Epoch 0, iter 260/787) Average loss so far: 6.047\n",
      "(Epoch 0, iter 270/787) Average loss so far: 6.051\n",
      "(Epoch 0, iter 280/787) Average loss so far: 6.060\n",
      "(Epoch 0, iter 290/787) Average loss so far: 6.066\n",
      "(Epoch 0, iter 300/787) Average loss so far: 6.049\n",
      "(Epoch 0, iter 310/787) Average loss so far: 6.079\n",
      "(Epoch 0, iter 320/787) Average loss so far: 6.036\n",
      "(Epoch 0, iter 330/787) Average loss so far: 6.086\n",
      "(Epoch 0, iter 340/787) Average loss so far: 6.053\n",
      "(Epoch 0, iter 350/787) Average loss so far: 6.057\n",
      "(Epoch 0, iter 360/787) Average loss so far: 6.065\n",
      "(Epoch 0, iter 370/787) Average loss so far: 6.035\n",
      "(Epoch 0, iter 380/787) Average loss so far: 6.068\n",
      "(Epoch 0, iter 390/787) Average loss so far: 6.032\n",
      "(Epoch 0, iter 400/787) Average loss so far: 6.023\n",
      "(Epoch 0, iter 410/787) Average loss so far: 6.059\n",
      "(Epoch 0, iter 420/787) Average loss so far: 6.049\n",
      "(Epoch 0, iter 430/787) Average loss so far: 6.041\n",
      "(Epoch 0, iter 440/787) Average loss so far: 6.036\n",
      "(Epoch 0, iter 450/787) Average loss so far: 6.030\n",
      "(Epoch 0, iter 460/787) Average loss so far: 6.024\n",
      "(Epoch 0, iter 470/787) Average loss so far: 6.024\n",
      "(Epoch 0, iter 480/787) Average loss so far: 6.035\n",
      "(Epoch 0, iter 490/787) Average loss so far: 6.048\n",
      "(Epoch 0, iter 500/787) Average loss so far: 6.031\n",
      "(Epoch 0, iter 510/787) Average loss so far: 6.033\n",
      "(Epoch 0, iter 520/787) Average loss so far: 6.041\n",
      "(Epoch 0, iter 530/787) Average loss so far: 6.030\n",
      "(Epoch 0, iter 540/787) Average loss so far: 6.028\n",
      "(Epoch 0, iter 550/787) Average loss so far: 6.011\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m dec_attn_scheduler \u001b[38;5;241m=\u001b[39m CosineAnnealingLR(decoder_attn_optimizer, T_max\u001b[38;5;241m=\u001b[39mn_epochs, eta_min\u001b[38;5;241m=\u001b[39mmin_lr)\n\u001b[1;32m     11\u001b[0m identifier\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_adam_without_intermediate_tags_wd0_lr1e-3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m attn_epoch_losses, attn_val_epoch_losses, attn_log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_attn_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_attn_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43menc_lr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_attn_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_lr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_attn_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdev_ds_val_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdev_ds_val_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_ds_val_met\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_ds_val_met\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midentifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_iter_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/train.py:171\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, dataset, n_epochs, vocab, decoder_mode, batch_size, enc_lr_scheduler, dec_lr_scheduler, dev_ds_val_loss, dev_ds_val_met, identifier, min_bleu_to_save, verbose, verbose_iter_interval)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mprint\u001b[39m(msg)\n\u001b[1;32m    170\u001b[0m     print_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 171\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mingredients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ming_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# remove later\u001b[39;49;00m\n\u001b[1;32m    175\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    177\u001b[0m print_epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/train.py:98\u001b[0m, in \u001b[0;36mtrain_iter\u001b[0;34m(ingredients, recipes, ing_lens, rec_lens, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, decoder_mode, prevent_pretrained_grad_update, vocab)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-teacher forcing is not implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     96\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 98\u001b[0m all_decoder_outs, all_gt \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_decoder_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_cell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mingredients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded_rec_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword2index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPAD_WORD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m all_decoder_outs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_decoder_outs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    103\u001b[0m all_gt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(all_gt, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/train.py:44\u001b[0m, in \u001b[0;36mtrain_decoder_iter\u001b[0;34m(decoder, decoder_hidden, decoder_cell, encoder_houts, ingredients, recipes, padded_rec_len, rec_lens, pad_word_idx, decoder_mode)\u001b[0m\n\u001b[1;32m     40\u001b[0m all_decoder_outs\u001b[38;5;241m.\u001b[39mappend(decoder_out)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# because we ensured that input cannot be end token, there is a guaranteed non-padding token\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# for each valid batch sample\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m gt_i \u001b[38;5;241m=\u001b[39m \u001b[43mrecipes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# [N_valid]\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (gt_i \u001b[38;5;241m!=\u001b[39m pad_word_idx)\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt_i should not have padding but got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgt_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m all_gt\u001b[38;5;241m.\u001b[39mappend(gt_i)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_lr=1e-4\n",
    "min_lr = 1e-5\n",
    "n_epochs = 30\n",
    "batch_size=128\n",
    "encoder_attn_optimizer = optim.Adam(encoder_attn.parameters(), lr=initial_lr)\n",
    "decoder_attn_optimizer = optim.Adam(decoder_attn.parameters(), lr=initial_lr)\n",
    "# enc_attn_scheduler = CosineAnnealingLR(encoder_attn_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "# dec_attn_scheduler = CosineAnnealingLR(decoder_attn_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "enc_attn_scheduler = CosineAnnealingLR(encoder_attn_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "dec_attn_scheduler = CosineAnnealingLR(decoder_attn_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "identifier=\"attn_adam_without_intermediate_tags_wd0_lr1e-3\"\n",
    "\n",
    "attn_epoch_losses, attn_val_epoch_losses, attn_log = train(\n",
    "    encoder_attn, decoder_attn, encoder_attn_optimizer, decoder_attn_optimizer, train_ds, \n",
    "    n_epochs=n_epochs, vocab=vocab, decoder_mode=\"attention\", batch_size=batch_size, \n",
    "    enc_lr_scheduler=enc_attn_scheduler, dec_lr_scheduler=dec_attn_scheduler, \n",
    "    dev_ds_val_loss = dev_ds_val_loss, dev_ds_val_met=dev_ds_val_met, identifier=identifier,\n",
    "    verbose_iter_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder (Extension: pretrained embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding_dict = create_pretrained_embedding_dict(\"./glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=300\n",
    "encoder_pretrained_embed = EncoderRNN(\n",
    "    input_size=vocab.n_unique_words, embedding_size=embedding_size, hidden_size=HIDDEN_SIZE, \n",
    "    padding_value=vocab.word2index(PAD_WORD), pretrained_embedding_dict=pretrained_embedding_dict, \n",
    "    vocab=vocab).to(DEVICE)\n",
    "# in the training script, decoder is always fed a non-end token and thus never needs to generate padding\n",
    "# also it should never generate \"<UNKNOWN>\"\n",
    "decoder_pretrained_embed = DecoderRNN(\n",
    "    embedding_size=embedding_size,hidden_size=HIDDEN_SIZE, output_size=vocab.n_unique_words-2,\n",
    "    pretrained_embedding_dict=pretrained_embedding_dict, vocab=vocab).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr=0.8\n",
    "min_lr = 0.01\n",
    "n_epochs = 20\n",
    "batch_size=128\n",
    "encoder_pretrained_embed_optimizer = optim.SGD(encoder_pretrained_embed.parameters(), lr=initial_lr)\n",
    "decoder_pretrained_embed_optimizer = optim.SGD(decoder_pretrained_embed.parameters(), lr=initial_lr)\n",
    "# enc_scheduler = CosineAnnealingLR(encoder_pretrained_embed_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "# dec_scheduler = CosineAnnealingLR(decoder_pretrained_embed_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "enc_pretrained_embed_scheduler = MultiStepLR(encoder_pretrained_embed_optimizer, milestones=[15], gamma=0.2)\n",
    "dec_pretrained_embed_scheduler = MultiStepLR(decoder_pretrained_embed_optimizer, milestones=[15], gamma=0.2)\n",
    "\n",
    "epoch_losses = train(encoder_pretrained_embed, decoder_pretrained_embed, \n",
    "                     encoder_pretrained_embed_optimizer, decoder_pretrained_embed_optimizer, recipe_ds, \n",
    "                     n_epochs=n_epochs, vocab=vocab, decoder_mode=\"basic\", batch_size=batch_size, \n",
    "                     enc_lr_scheduler=enc_pretrained_embed_scheduler, dec_lr_scheduler=dec_pretrained_embed_scheduler, \n",
    "                     verbose_iter_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr=0.8\n",
    "min_lr = 0.01\n",
    "n_epochs = 20\n",
    "batch_size=128\n",
    "encoder_pretrained_embed_optimizer = optim.SGD(encoder_pretrained_embed.parameters(), lr=initial_lr)\n",
    "decoder_pretrained_embed_optimizer = optim.SGD(decoder_pretrained_embed.parameters(), lr=initial_lr)\n",
    "# enc_scheduler = CosineAnnealingLR(encoder_pretrained_embed_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "# dec_scheduler = CosineAnnealingLR(decoder_pretrained_embed_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "enc_pretrained_embed_scheduler = MultiStepLR(encoder_pretrained_embed_optimizer, milestones=[15], gamma=0.2)\n",
    "dec_pretrained_embed_scheduler = MultiStepLR(decoder_pretrained_embed_optimizer, milestones=[15], gamma=0.2)\n",
    "\n",
    "epoch_losses = train(encoder_pretrained_embed, decoder_pretrained_embed, \n",
    "                     encoder_pretrained_embed_optimizer, decoder_pretrained_embed_optimizer, recipe_ds, \n",
    "                     n_epochs=n_epochs, vocab=vocab, decoder_mode=\"basic\", batch_size=batch_size, \n",
    "                     enc_lr_scheduler=enc_pretrained_embed_scheduler, dec_lr_scheduler=dec_pretrained_embed_scheduler, \n",
    "                     verbose_iter_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_attn = AttnDecoderRNN(embedding_size=embedding_size, hidden_size=HIDDEN_SIZE,\n",
    "#                               output_size=vocab.n_unique_words-2, \n",
    "#                               padding_val=vocab.word2index(PAD_WORD)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(recipe_ds, batch_size=4, shuffle=True, collate_fn=pad_collate(vocab))\n",
    "# ingredients, recipes, ing_lens, rec_lens = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rec_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_lr=0.8\n",
    "# min_lr = 0.01\n",
    "# n_epochs = 30\n",
    "# batch_size=128\n",
    "# encoder_optimizer = optim.SGD(encoder.parameters(), lr=initial_lr)\n",
    "# decoder_optimizer = optim.SGD(decoder.parameters(), lr=initial_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter(ingredients, recipes, ing_lens, rec_lens, encoder, decoder_attn, \n",
    "#            encoder_optimizer, decoder_optimizer, criterion=nn.NLLLoss(),\n",
    "#            decoder_mode=\"attention\", vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(encoder, decoder, \"adam_with_intermediate_tags_wd0_lr1e-3_ep_9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:15<00:00, 11.88it/s]\n"
     ]
    }
   ],
   "source": [
    "all_decoder_outs, all_gt_recipes = eval(encoder, decoder, test_ds, vocab,\n",
    "                                        max_recipe_len=MAX_RECIPE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<RECIPE_START>',\n",
       " 'mix',\n",
       " 'the',\n",
       " 'crushed',\n",
       " 'graham',\n",
       " 'crackers',\n",
       " ',',\n",
       " '2',\n",
       " 'tsp',\n",
       " '<RECIPE_STEP>',\n",
       " 'of',\n",
       " 'cinnamon',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'brown',\n",
       " 'sugar',\n",
       " 'or',\n",
       " 'substitute',\n",
       " 'together',\n",
       " '<RECIPE_STEP>',\n",
       " 'stir',\n",
       " 'in',\n",
       " 'the',\n",
       " 'melted',\n",
       " 'margarine',\n",
       " 'and',\n",
       " 'set',\n",
       " 'the',\n",
       " 'mixture',\n",
       " 'to',\n",
       " 'one',\n",
       " 'side',\n",
       " '<RECIPE_STEP>',\n",
       " 'mix',\n",
       " 'the',\n",
       " 'sliced',\n",
       " 'apples',\n",
       " ',',\n",
       " '5',\n",
       " 'tsp',\n",
       " '<RECIPE_STEP>',\n",
       " 'of',\n",
       " 'cinnamon',\n",
       " ',',\n",
       " 'corn',\n",
       " 'starch',\n",
       " ',',\n",
       " 'and',\n",
       " '1',\n",
       " 'cup',\n",
       " 'of',\n",
       " 'brown',\n",
       " 'sugar',\n",
       " 'or',\n",
       " 'substitute',\n",
       " 'together',\n",
       " 'and',\n",
       " 'pour',\n",
       " 'into',\n",
       " 'a',\n",
       " '13',\n",
       " \"''\",\n",
       " 'x',\n",
       " '9',\n",
       " \"''\",\n",
       " 'pan',\n",
       " '<RECIPE_STEP>',\n",
       " 'sprinkle',\n",
       " 'on',\n",
       " 'the',\n",
       " 'topping',\n",
       " 'and',\n",
       " 'pat',\n",
       " 'down',\n",
       " '<RECIPE_STEP>',\n",
       " 'bake',\n",
       " '35',\n",
       " 'minutes',\n",
       " 'in',\n",
       " 'an',\n",
       " 'oven',\n",
       " 'preheated',\n",
       " 'to',\n",
       " '350',\n",
       " 'degrees',\n",
       " '<RECIPE_STEP>',\n",
       " 'let',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'enjoy',\n",
       " '!',\n",
       " '!',\n",
       " '<RECIPE_END>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_gt_recipes[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<RECIPE_START>',\n",
       " 'combine',\n",
       " 'all',\n",
       " 'ingredients',\n",
       " 'except',\n",
       " 'nuts',\n",
       " '<RECIPE_STEP>',\n",
       " 'in',\n",
       " 'a',\n",
       " 'large',\n",
       " 'bowl',\n",
       " ',',\n",
       " 'combine',\n",
       " 'butter',\n",
       " ',',\n",
       " 'sugar',\n",
       " ',',\n",
       " 'and',\n",
       " 'vanilla',\n",
       " '<RECIPE_STEP>',\n",
       " 'mix',\n",
       " 'well',\n",
       " '<RECIPE_STEP>',\n",
       " 'pour',\n",
       " 'into',\n",
       " 'greased',\n",
       " '9',\n",
       " \"''\",\n",
       " 'x',\n",
       " '13',\n",
       " \"''\",\n",
       " 'baking',\n",
       " 'pan',\n",
       " '<RECIPE_STEP>',\n",
       " 'bake',\n",
       " 'at',\n",
       " '350',\n",
       " 'degrees',\n",
       " 'for',\n",
       " '30',\n",
       " 'minutes',\n",
       " '<RECIPE_STEP>',\n",
       " 'cool',\n",
       " '<RECIPE_STEP>',\n",
       " '<RECIPE_END>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_decoder_outs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04852470977830739"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_bleu(all_gt_recipes, all_decoder_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_meteor(all_gt_recipes, all_decoder_outs, split_gt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ings = get_all_ingredients(\"./ingredient_set.json\")\n",
    "all_ings_regex = get_ingredients_regex(all_ings)\n",
    "metric_sample_ings, metric_sample_gold_recipe, metric_sample_generated_recipe = \\\n",
    "    load_metric_sample(\"./metric_sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_inp_ings, n_extra_ings = get_prop_input_num_extra_ingredients(\n",
    "    metric_sample_ings, metric_sample_generated_recipe, all_ings_regex, verbose=True,\n",
    "    metric_sample=True)\n",
    "print(f\"\\nproportion of input ingredients: {prop_inp_ings}\\nnumber of extra ingredients: {n_extra_ings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = calc_bleu([metric_sample_gold_recipe], [metric_sample_generated_recipe], split_gen=True)\n",
    "meteor_score = calc_meteor([metric_sample_gold_recipe], [metric_sample_generated_recipe], split_gen=True)\n",
    "print(f\"BLEU score: {bleu_score}, METEOR score: {meteor_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
