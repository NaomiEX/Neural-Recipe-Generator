{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "# !pip install matplotlib numpy pandas tqdm nltk\n",
    "\n",
    "# for separating ingredients vs non-ingredients\n",
    "# NOTE: if using Windows to run this, need to download GNU Wget\n",
    "# !wget -c https://raw.githubusercontent.com/williamLyh/RecipeWithPlans/main/ingredient_set.json -O ingredient_set.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junn/miniconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, MultiStepLR, CosineAnnealingWarmRestarts\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from nltk.translate import meteor\n",
    "\n",
    "from data import *\n",
    "from encoder_decoder import *\n",
    "from train import *\n",
    "from eval import *\n",
    "from utils import *\n",
    "\n",
    "# required for bleu\n",
    "# nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 31989101\n",
    "HIDDEN_SIZE = 256\n",
    "MAX_INGR_LEN = 150 # fixed from assignment\n",
    "MAX_RECIPE_LEN = 600\n",
    "DROPOUT = 0.1\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## ensuring reproducibility\n",
    "def reset_rng():\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "reset_rng()\n",
    "\n",
    "# to easily read ingredients and instructions\n",
    "pd.set_option('display.max_colwidth', 2000)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./Cooking_Dataset\"\n",
    "add_intermediate_tag=True\n",
    "\n",
    "train_df_orig = pd.read_csv(os.path.join(data_root, \"train.csv\"), usecols=['Ingredients', 'Recipe'])\n",
    "dev_df_orig = pd.read_csv(os.path.join(data_root, \"dev.csv\"), usecols=['Ingredients', 'Recipe'])\n",
    "test_df_orig = pd.read_csv(os.path.join(data_root, \"test.csv\"), usecols=['Ingredients', 'Recipe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples before preprocessing: 101340\n",
      "Number of data samples after preprocessing: 99035 (97.725%)\n"
     ]
    }
   ],
   "source": [
    "train_df = preprocess_data(train_df_orig, max_ingr_len=MAX_INGR_LEN, max_recipe_len=MAX_RECIPE_LEN, add_intermediate_tag=add_intermediate_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples before preprocessing: 797\n",
      "Number of data samples after preprocessing: 775 (97.240%)\n"
     ]
    }
   ],
   "source": [
    "dev_df = preprocess_data(dev_df_orig, max_ingr_len=MAX_INGR_LEN, max_recipe_len=MAX_RECIPE_LEN, add_intermediate_tag=add_intermediate_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples before preprocessing: 778\n",
      "Number of data samples after preprocessing: 757 (97.301%)\n"
     ]
    }
   ],
   "source": [
    "test_df = preprocess_data(test_df_orig, max_ingr_len=MAX_INGR_LEN, max_recipe_len=MAX_RECIPE_LEN, add_intermediate_tag=add_intermediate_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 11907/99035 [00:00<00:02, 30171.81it/s]"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary(add_intermediate_tag=add_intermediate_tag)\n",
    "vocab.populate(train_df)\n",
    "vocab.n_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = Vocabulary()\n",
    "# vocab.populate(train_df)\n",
    "# vocab.n_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = Vocabulary()\n",
    "# vocab.populate(train_df)\n",
    "# vocab.n_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RecipeDataset(train_df, vocab)\n",
    "dev_ds = RecipeDataset(dev_df, vocab, train=False)\n",
    "test_ds = RecipeDataset(test_df, vocab, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder (Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=300\n",
    "encoder = EncoderRNN(vocab.n_unique_words, embedding_size=embedding_size, hidden_size=HIDDEN_SIZE, padding_value=vocab.word2index(PAD_WORD)).to(DEVICE)\n",
    "# in the training script, decoder is always fed a non-end token and thus never needs to generate padding\n",
    "# also it should never generate \"<UNKNOWN>\"\n",
    "decoder = DecoderRNN(embedding_size=embedding_size,hidden_size=HIDDEN_SIZE, output_size=vocab.n_unique_words-2).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/20, enc lr scheduler: [0.001], dec lr scheduler: [0.001]\n",
      "(Epoch 0, iter 50/774) Average loss so far: 6.980\n",
      "(Epoch 0, iter 100/774) Average loss so far: 5.635\n",
      "(Epoch 0, iter 150/774) Average loss so far: 5.343\n",
      "(Epoch 0, iter 200/774) Average loss so far: 5.109\n",
      "(Epoch 0, iter 250/774) Average loss so far: 4.883\n",
      "(Epoch 0, iter 300/774) Average loss so far: 4.716\n",
      "(Epoch 0, iter 350/774) Average loss so far: 4.561\n",
      "(Epoch 0, iter 400/774) Average loss so far: 4.420\n",
      "(Epoch 0, iter 450/774) Average loss so far: 4.319\n",
      "(Epoch 0, iter 500/774) Average loss so far: 4.236\n",
      "(Epoch 0, iter 550/774) Average loss so far: 4.165\n",
      "(Epoch 0, iter 600/774) Average loss so far: 4.095\n",
      "(Epoch 0, iter 650/774) Average loss so far: 4.044\n",
      "(Epoch 0, iter 700/774) Average loss so far: 3.970\n",
      "(Epoch 0, iter 750/774) Average loss so far: 3.927\n",
      "Average epoch loss: 4.669\n",
      "This epoch took 6.056725811958313 mins. Time remaining: 1.0 hrs 55.0 mins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:34<00:00,  5.57it/s]\n",
      "100%|██████████| 775/775 [00:03<00:00, 224.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.011372860255505471, METEOR score: 0.1453281774723996\n",
      "Starting epoch 2/20, enc lr scheduler: [0.0009939057285945933], dec lr scheduler: [0.0009939057285945933]\n",
      "(Epoch 1, iter 50/774) Average loss so far: 3.879\n",
      "(Epoch 1, iter 100/774) Average loss so far: 3.848\n",
      "(Epoch 1, iter 150/774) Average loss so far: 3.814\n",
      "(Epoch 1, iter 200/774) Average loss so far: 3.787\n",
      "(Epoch 1, iter 250/774) Average loss so far: 3.768\n",
      "(Epoch 1, iter 300/774) Average loss so far: 3.742\n",
      "(Epoch 1, iter 350/774) Average loss so far: 3.719\n",
      "(Epoch 1, iter 400/774) Average loss so far: 3.710\n",
      "(Epoch 1, iter 450/774) Average loss so far: 3.671\n",
      "(Epoch 1, iter 500/774) Average loss so far: 3.657\n",
      "(Epoch 1, iter 550/774) Average loss so far: 3.652\n",
      "(Epoch 1, iter 600/774) Average loss so far: 3.638\n",
      "(Epoch 1, iter 650/774) Average loss so far: 3.630\n",
      "(Epoch 1, iter 700/774) Average loss so far: 3.604\n",
      "(Epoch 1, iter 750/774) Average loss so far: 3.607\n",
      "Average epoch loss: 3.712\n",
      "This epoch took 6.06739669640859 mins. Time remaining: 1.0 hrs 49.0 mins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:45<00:00,  4.26it/s]\n",
      "100%|██████████| 775/775 [00:04<00:00, 176.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.007864949403355501, METEOR score: 0.11481570115071774\n",
      "Starting epoch 3/20, enc lr scheduler: [0.0009757729755661012], dec lr scheduler: [0.0009757729755661012]\n",
      "(Epoch 2, iter 50/774) Average loss so far: 3.581\n",
      "(Epoch 2, iter 100/774) Average loss so far: 3.582\n",
      "(Epoch 2, iter 150/774) Average loss so far: 3.574\n",
      "(Epoch 2, iter 200/774) Average loss so far: 3.553\n",
      "(Epoch 2, iter 250/774) Average loss so far: 3.581\n",
      "(Epoch 2, iter 300/774) Average loss so far: 3.555\n",
      "(Epoch 2, iter 350/774) Average loss so far: 3.543\n",
      "(Epoch 2, iter 400/774) Average loss so far: 3.564\n",
      "(Epoch 2, iter 450/774) Average loss so far: 3.529\n",
      "(Epoch 2, iter 500/774) Average loss so far: 3.540\n",
      "(Epoch 2, iter 550/774) Average loss so far: 3.548\n",
      "(Epoch 2, iter 600/774) Average loss so far: 3.535\n",
      "(Epoch 2, iter 650/774) Average loss so far: 3.545\n",
      "(Epoch 2, iter 700/774) Average loss so far: 3.530\n",
      "(Epoch 2, iter 750/774) Average loss so far: 3.544\n",
      "Average epoch loss: 3.553\n",
      "This epoch took 6.067815732955933 mins. Time remaining: 1.0 hrs 43.0 mins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:42<00:00,  4.62it/s]\n",
      "100%|██████████| 775/775 [00:03<00:00, 208.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.01084420594557128, METEOR score: 0.14220265956255074\n",
      "Starting epoch 4/20, enc lr scheduler: [0.0009460482294732422], dec lr scheduler: [0.0009460482294732422]\n",
      "(Epoch 3, iter 50/774) Average loss so far: 3.519\n",
      "(Epoch 3, iter 100/774) Average loss so far: 3.540\n",
      "(Epoch 3, iter 150/774) Average loss so far: 3.563\n",
      "(Epoch 3, iter 200/774) Average loss so far: 3.570\n",
      "(Epoch 3, iter 250/774) Average loss so far: 3.551\n",
      "(Epoch 3, iter 300/774) Average loss so far: 3.517\n",
      "(Epoch 3, iter 350/774) Average loss so far: 3.548\n",
      "(Epoch 3, iter 400/774) Average loss so far: 3.562\n",
      "(Epoch 3, iter 450/774) Average loss so far: 3.553\n",
      "(Epoch 3, iter 500/774) Average loss so far: 3.555\n",
      "(Epoch 3, iter 550/774) Average loss so far: 3.528\n",
      "(Epoch 3, iter 600/774) Average loss so far: 3.582\n",
      "(Epoch 3, iter 650/774) Average loss so far: 3.555\n",
      "(Epoch 3, iter 700/774) Average loss so far: 3.596\n",
      "(Epoch 3, iter 750/774) Average loss so far: 3.584\n",
      "Average epoch loss: 3.556\n",
      "This epoch took 6.016711568832397 mins. Time remaining: 1.0 hrs 36.0 mins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:26<00:00,  7.37it/s]\n",
      "100%|██████████| 775/775 [00:02<00:00, 364.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.023459973733049522, METEOR score: 0.16159777091583263\n",
      "Starting epoch 5/20, enc lr scheduler: [0.0009054634122155991], dec lr scheduler: [0.0009054634122155991]\n",
      "(Epoch 4, iter 50/774) Average loss so far: 3.603\n",
      "(Epoch 4, iter 100/774) Average loss so far: 3.599\n",
      "(Epoch 4, iter 150/774) Average loss so far: 3.610\n",
      "(Epoch 4, iter 200/774) Average loss so far: 3.623\n",
      "(Epoch 4, iter 250/774) Average loss so far: 3.650\n",
      "(Epoch 4, iter 300/774) Average loss so far: 3.634\n",
      "(Epoch 4, iter 350/774) Average loss so far: 3.638\n",
      "(Epoch 4, iter 400/774) Average loss so far: 3.644\n",
      "(Epoch 4, iter 450/774) Average loss so far: 3.662\n",
      "(Epoch 4, iter 500/774) Average loss so far: 3.683\n",
      "(Epoch 4, iter 550/774) Average loss so far: 3.706\n",
      "(Epoch 4, iter 600/774) Average loss so far: 3.705\n",
      "(Epoch 4, iter 650/774) Average loss so far: 3.708\n",
      "(Epoch 4, iter 700/774) Average loss so far: 3.742\n",
      "(Epoch 4, iter 750/774) Average loss so far: 3.742\n",
      "Average epoch loss: 3.666\n",
      "This epoch took 6.07977124452591 mins. Time remaining: 1.0 hrs 31.0 mins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [00:37<00:00,  5.15it/s]\n",
      "100%|██████████| 775/775 [00:03<00:00, 212.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.017233810026183283, METEOR score: 0.16694116378968918\n",
      "Starting epoch 6/20, enc lr scheduler: [0.0008550178566873411], dec lr scheduler: [0.0008550178566873411]\n",
      "(Epoch 5, iter 50/774) Average loss so far: 3.763\n",
      "(Epoch 5, iter 100/774) Average loss so far: 3.771\n",
      "(Epoch 5, iter 150/774) Average loss so far: 3.804\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m dec_scheduler \u001b[38;5;241m=\u001b[39m CosineAnnealingLR(decoder_optimizer, T_max\u001b[38;5;241m=\u001b[39mn_epochs, eta_min\u001b[38;5;241m=\u001b[39mmin_lr)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# enc_scheduler = MultiStepLR(encoder_optimizer, milestones=[15], gamma=0.1)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# dec_scheduler = MultiStepLR(decoder_optimizer, milestones=[15], gamma=0.1)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbasic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                     \u001b[49m\u001b[43menc_lr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_lr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdev_ds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdev_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midentifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madam_without_intermediate_tags_wd1e-5_lr0.01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverbose_iter_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Michelle/neural-recipe-generator/train.py:165\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, dataset, n_epochs, vocab, decoder_mode, batch_size, enc_lr_scheduler, dec_lr_scheduler, dev_ds, identifier, min_bleu_to_save, verbose, verbose_iter_interval)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miter_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_iters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) Average loss so far: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprint_epoch_loss\u001b[38;5;241m/\u001b[39mverbose_iter_interval\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m     print_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 165\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mingredients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ming_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# remove later\u001b[39;49;00m\n\u001b[1;32m    169\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    171\u001b[0m print_epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/Michelle/neural-recipe-generator/train.py:109\u001b[0m, in \u001b[0;36mtrain_iter\u001b[0;34m(ingredients, recipes, ing_lens, rec_lens, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, decoder_mode, prevent_pretrained_grad_update, vocab)\u001b[0m\n\u001b[1;32m    106\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(all_decoder_outs, all_gt)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m## backpropagation\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# print(\"=====BEFORE MASKING=====\")\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# print(\"ENCODER EMBEDDING WEIGHT GRADS of shape:\", encoder.embedding.weight.grad.shape)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# print(encoder.embedding.weight.grad[:10, :10])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# print(\"=====AFTER MASKING=====\")\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prevent_pretrained_grad_update:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_lr=1e-3\n",
    "min_lr = 1e-5\n",
    "n_epochs = 20\n",
    "batch_size=128\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=initial_lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=initial_lr)\n",
    "# enc_scheduler = CosineAnnealingWarmRestarts(encoder_optimizer, T_0=math.ceil(len(train_ds) / 128), \n",
    "#                                             verbose=True, eta_min=min_lr)\n",
    "# dec_scheduler = CosineAnnealingWarmRestarts(decoder_optimizer, T_0=math.ceil(len(train_ds) / 128), \n",
    "#                                             verbose=True, eta_min=min_lr)\n",
    "enc_scheduler = CosineAnnealingLR(encoder_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "dec_scheduler = CosineAnnealingLR(decoder_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "# enc_scheduler = MultiStepLR(encoder_optimizer, milestones=[15], gamma=0.1)\n",
    "# dec_scheduler = MultiStepLR(decoder_optimizer, milestones=[15], gamma=0.1)\n",
    "\n",
    "epoch_losses = train(encoder, decoder, encoder_optimizer, decoder_optimizer, train_ds, \n",
    "                     n_epochs=n_epochs, vocab=vocab, decoder_mode=\"basic\", batch_size=batch_size, \n",
    "                     enc_lr_scheduler=enc_scheduler, dec_lr_scheduler=dec_scheduler, \n",
    "                     dev_ds = dev_ds, identifier=\"adam_without_intermediate_tags_wd1e-5_lr0.01\",\n",
    "                     verbose_iter_interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder (Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=300\n",
    "encoder_attn = EncoderRNN(vocab.n_unique_words, embedding_size=embedding_size, hidden_size=HIDDEN_SIZE, padding_value=vocab.word2index(PAD_WORD)).to(DEVICE)\n",
    "# in the training script, decoder is always fed a non-end token and thus never needs to generate padding\n",
    "# also it should never generate \"<UNKNOWN>\"\n",
    "# decoder = DecoderRNN(embedding_size=embedding_size,hidden_size=HIDDEN_SIZE, output_size=vocab.n_unique_words-2).to(DEVICE)\n",
    "decoder_attn = AttnDecoderRNN(embedding_size, hidden_size=HIDDEN_SIZE, output_size=vocab.n_unique_words-2, padding_val=vocab.word2index(PAD_WORD), dropout=DROPOUT).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/30, enc lr scheduler: [0.8], dec lr scheduler: [0.8]\n",
      "(Epoch 0, iter 10/774) Average loss so far: 10.356\n",
      "(Epoch 0, iter 20/774) Average loss so far: 8.718\n",
      "(Epoch 0, iter 30/774) Average loss so far: 7.806\n",
      "(Epoch 0, iter 40/774) Average loss so far: 7.802\n",
      "(Epoch 0, iter 50/774) Average loss so far: 7.435\n",
      "(Epoch 0, iter 60/774) Average loss so far: 7.306\n",
      "(Epoch 0, iter 70/774) Average loss so far: 6.935\n",
      "(Epoch 0, iter 80/774) Average loss so far: 6.662\n",
      "(Epoch 0, iter 90/774) Average loss so far: 6.400\n",
      "(Epoch 0, iter 100/774) Average loss so far: 6.532\n",
      "(Epoch 0, iter 110/774) Average loss so far: 6.366\n",
      "(Epoch 0, iter 120/774) Average loss so far: 6.335\n",
      "(Epoch 0, iter 130/774) Average loss so far: 6.155\n",
      "(Epoch 0, iter 140/774) Average loss so far: 6.100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m enc_attn_scheduler \u001b[38;5;241m=\u001b[39m MultiStepLR(encoder_attn_optimizer, milestones\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m15\u001b[39m], gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     10\u001b[0m dec_attn_scheduler \u001b[38;5;241m=\u001b[39m MultiStepLR(decoder_attn_optimizer, milestones\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m15\u001b[39m], gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_attn_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_attn_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipe_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                     \u001b[49m\u001b[43menc_lr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_attn_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_lr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_attn_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverbose_iter_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/train.py:141\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, dataset, n_epochs, vocab, decoder_mode, batch_size, enc_lr_scheduler, dec_lr_scheduler, verbose, verbose_iter_interval)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miter_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_iters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) Average loss so far: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprint_epoch_loss\u001b[38;5;241m/\u001b[39mverbose_iter_interval\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m     print_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 141\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mingredients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ming_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# remove later\u001b[39;49;00m\n\u001b[1;32m    145\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    147\u001b[0m print_epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/train.py:111\u001b[0m, in \u001b[0;36mtrain_iter\u001b[0;34m(ingredients, recipes, ing_lens, rec_lens, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, decoder_mode, vocab)\u001b[0m\n\u001b[1;32m    108\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(all_decoder_outs, all_gt)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m## backpropagation\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m## update params\u001b[39;00m\n\u001b[1;32m    114\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_lr=0.8\n",
    "min_lr = 0.01\n",
    "n_epochs = 30\n",
    "batch_size=128\n",
    "encoder_attn_optimizer = optim.SGD(encoder_attn.parameters(), lr=initial_lr)\n",
    "decoder_attn_optimizer = optim.SGD(decoder_attn.parameters(), lr=initial_lr)\n",
    "# enc_attn_scheduler = CosineAnnealingLR(encoder_attn_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "# dec_attn_scheduler = CosineAnnealingLR(decoder_attn_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "enc_attn_scheduler = MultiStepLR(encoder_attn_optimizer, milestones=[15], gamma=0.1)\n",
    "dec_attn_scheduler = MultiStepLR(decoder_attn_optimizer, milestones=[15], gamma=0.1)\n",
    "\n",
    "epoch_losses = train(encoder_attn, decoder_attn, encoder_attn_optimizer, decoder_attn_optimizer, recipe_ds, \n",
    "                     n_epochs=n_epochs, vocab=vocab, decoder_mode=\"attention\", batch_size=batch_size, \n",
    "                     enc_lr_scheduler=enc_attn_scheduler, dec_lr_scheduler=dec_attn_scheduler, \n",
    "                     verbose_iter_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder (Extension: pretrained embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding_dict = create_pretrained_embedding_dict(\"./glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42650 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42650/42650 [00:00<00:00, 386465.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28250/42650 (0.662) words have pretrained embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42648/42648 [00:00<00:00, 360630.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28250/42650 (0.662) words have pretrained embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_size=300\n",
    "encoder_pretrained_embed = EncoderRNN(\n",
    "    input_size=vocab.n_unique_words, embedding_size=embedding_size, hidden_size=HIDDEN_SIZE, \n",
    "    padding_value=vocab.word2index(PAD_WORD), pretrained_embedding_dict=pretrained_embedding_dict, \n",
    "    vocab=vocab).to(DEVICE)\n",
    "# in the training script, decoder is always fed a non-end token and thus never needs to generate padding\n",
    "# also it should never generate \"<UNKNOWN>\"\n",
    "decoder_pretrained_embed = DecoderRNN(\n",
    "    embedding_size=embedding_size,hidden_size=HIDDEN_SIZE, output_size=vocab.n_unique_words-2,\n",
    "    pretrained_embedding_dict=pretrained_embedding_dict, vocab=vocab).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/20, enc lr scheduler: [0.8], dec lr scheduler: [0.8]\n",
      "(Epoch 0, iter 10/774) Average loss so far: 10.462\n",
      "(Epoch 0, iter 20/774) Average loss so far: 8.980\n",
      "(Epoch 0, iter 30/774) Average loss so far: 8.117\n",
      "(Epoch 0, iter 40/774) Average loss so far: 7.807\n",
      "(Epoch 0, iter 50/774) Average loss so far: 7.399\n",
      "(Epoch 0, iter 60/774) Average loss so far: 7.265\n",
      "(Epoch 0, iter 70/774) Average loss so far: 6.978\n",
      "(Epoch 0, iter 80/774) Average loss so far: 6.980\n",
      "(Epoch 0, iter 90/774) Average loss so far: 6.677\n",
      "(Epoch 0, iter 100/774) Average loss so far: 6.663\n",
      "(Epoch 0, iter 110/774) Average loss so far: 6.488\n",
      "(Epoch 0, iter 120/774) Average loss so far: 6.277\n",
      "(Epoch 0, iter 130/774) Average loss so far: 6.548\n",
      "(Epoch 0, iter 140/774) Average loss so far: 6.164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m enc_pretrained_embed_scheduler \u001b[38;5;241m=\u001b[39m MultiStepLR(encoder_pretrained_embed_optimizer, milestones\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m15\u001b[39m], gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m     10\u001b[0m dec_pretrained_embed_scheduler \u001b[38;5;241m=\u001b[39m MultiStepLR(decoder_pretrained_embed_optimizer, milestones\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m15\u001b[39m], gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_pretrained_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_pretrained_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mencoder_pretrained_embed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_pretrained_embed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipe_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbasic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                     \u001b[49m\u001b[43menc_lr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_pretrained_embed_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_lr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_pretrained_embed_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverbose_iter_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/train.py:154\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, dataset, n_epochs, vocab, decoder_mode, batch_size, enc_lr_scheduler, dec_lr_scheduler, verbose, verbose_iter_interval)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miter_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_iters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) Average loss so far: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprint_epoch_loss\u001b[38;5;241m/\u001b[39mverbose_iter_interval\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    153\u001b[0m     print_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 154\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mingredients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ming_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# remove later\u001b[39;49;00m\n\u001b[1;32m    158\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    160\u001b[0m print_epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/train.py:106\u001b[0m, in \u001b[0;36mtrain_iter\u001b[0;34m(ingredients, recipes, ing_lens, rec_lens, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, decoder_mode, prevent_pretrained_grad_update, vocab)\u001b[0m\n\u001b[1;32m    103\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(all_decoder_outs, all_gt)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m## backpropagation\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# print(\"=====BEFORE MASKING=====\")\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# print(\"ENCODER EMBEDDING WEIGHT GRADS of shape:\", encoder.embedding.weight.grad.shape)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# print(encoder.embedding.weight.grad[:10, :10])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# print(\"=====AFTER MASKING=====\")\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prevent_pretrained_grad_update:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_lr=0.8\n",
    "min_lr = 0.01\n",
    "n_epochs = 20\n",
    "batch_size=128\n",
    "encoder_pretrained_embed_optimizer = optim.SGD(encoder_pretrained_embed.parameters(), lr=initial_lr)\n",
    "decoder_pretrained_embed_optimizer = optim.SGD(decoder_pretrained_embed.parameters(), lr=initial_lr)\n",
    "# enc_scheduler = CosineAnnealingLR(encoder_pretrained_embed_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "# dec_scheduler = CosineAnnealingLR(decoder_pretrained_embed_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "enc_pretrained_embed_scheduler = MultiStepLR(encoder_pretrained_embed_optimizer, milestones=[15], gamma=0.2)\n",
    "dec_pretrained_embed_scheduler = MultiStepLR(decoder_pretrained_embed_optimizer, milestones=[15], gamma=0.2)\n",
    "\n",
    "epoch_losses = train(encoder_pretrained_embed, decoder_pretrained_embed, \n",
    "                     encoder_pretrained_embed_optimizer, decoder_pretrained_embed_optimizer, recipe_ds, \n",
    "                     n_epochs=n_epochs, vocab=vocab, decoder_mode=\"basic\", batch_size=batch_size, \n",
    "                     enc_lr_scheduler=enc_pretrained_embed_scheduler, dec_lr_scheduler=dec_pretrained_embed_scheduler, \n",
    "                     verbose_iter_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/20, enc lr scheduler: [0.8], dec lr scheduler: [0.8]\n",
      "(Epoch 0, iter 10/774) Average loss so far: 10.469\n",
      "(Epoch 0, iter 20/774) Average loss so far: 9.036\n",
      "(Epoch 0, iter 30/774) Average loss so far: 8.055\n",
      "(Epoch 0, iter 40/774) Average loss so far: 7.621\n",
      "(Epoch 0, iter 50/774) Average loss so far: 7.625\n",
      "(Epoch 0, iter 60/774) Average loss so far: 7.021\n",
      "(Epoch 0, iter 70/774) Average loss so far: 6.919\n",
      "(Epoch 0, iter 80/774) Average loss so far: 7.049\n",
      "(Epoch 0, iter 90/774) Average loss so far: 6.751\n",
      "(Epoch 0, iter 100/774) Average loss so far: 6.528\n",
      "(Epoch 0, iter 110/774) Average loss so far: 6.701\n",
      "(Epoch 0, iter 120/774) Average loss so far: 6.385\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m enc_pretrained_embed_scheduler \u001b[38;5;241m=\u001b[39m MultiStepLR(encoder_pretrained_embed_optimizer, milestones\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m15\u001b[39m], gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m     10\u001b[0m dec_pretrained_embed_scheduler \u001b[38;5;241m=\u001b[39m MultiStepLR(decoder_pretrained_embed_optimizer, milestones\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m15\u001b[39m], gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_pretrained_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_pretrained_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mencoder_pretrained_embed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_pretrained_embed_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipe_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbasic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                     \u001b[49m\u001b[43menc_lr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_pretrained_embed_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_lr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_pretrained_embed_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mverbose_iter_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/train.py:136\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, dataset, n_epochs, vocab, decoder_mode, batch_size, enc_lr_scheduler, dec_lr_scheduler, verbose, verbose_iter_interval)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miter_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_iters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) Average loss so far: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprint_epoch_loss\u001b[38;5;241m/\u001b[39mverbose_iter_interval\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m     print_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 136\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mingredients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ming_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdecoder_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# remove later\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    142\u001b[0m print_epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/train.py:106\u001b[0m, in \u001b[0;36mtrain_iter\u001b[0;34m(ingredients, recipes, ing_lens, rec_lens, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, decoder_mode, vocab)\u001b[0m\n\u001b[1;32m    103\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(all_decoder_outs, all_gt)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m## backpropagation\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m## update params\u001b[39;00m\n\u001b[1;32m    109\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_lr=0.8\n",
    "min_lr = 0.01\n",
    "n_epochs = 20\n",
    "batch_size=128\n",
    "encoder_pretrained_embed_optimizer = optim.SGD(encoder_pretrained_embed.parameters(), lr=initial_lr)\n",
    "decoder_pretrained_embed_optimizer = optim.SGD(decoder_pretrained_embed.parameters(), lr=initial_lr)\n",
    "# enc_scheduler = CosineAnnealingLR(encoder_pretrained_embed_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "# dec_scheduler = CosineAnnealingLR(decoder_pretrained_embed_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "enc_pretrained_embed_scheduler = MultiStepLR(encoder_pretrained_embed_optimizer, milestones=[15], gamma=0.2)\n",
    "dec_pretrained_embed_scheduler = MultiStepLR(decoder_pretrained_embed_optimizer, milestones=[15], gamma=0.2)\n",
    "\n",
    "epoch_losses = train(encoder_pretrained_embed, decoder_pretrained_embed, \n",
    "                     encoder_pretrained_embed_optimizer, decoder_pretrained_embed_optimizer, recipe_ds, \n",
    "                     n_epochs=n_epochs, vocab=vocab, decoder_mode=\"basic\", batch_size=batch_size, \n",
    "                     enc_lr_scheduler=enc_pretrained_embed_scheduler, dec_lr_scheduler=dec_pretrained_embed_scheduler, \n",
    "                     verbose_iter_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_attn = AttnDecoderRNN(embedding_size=embedding_size, hidden_size=HIDDEN_SIZE,\n",
    "#                               output_size=vocab.n_unique_words-2, \n",
    "#                               padding_val=vocab.word2index(PAD_WORD)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(recipe_ds, batch_size=4, shuffle=True, collate_fn=pad_collate(vocab))\n",
    "# ingredients, recipes, ing_lens, rec_lens = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 30, 164, 150,  20], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rec_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_lr=0.8\n",
    "# min_lr = 0.01\n",
    "# n_epochs = 30\n",
    "# batch_size=128\n",
    "# encoder_optimizer = optim.SGD(encoder.parameters(), lr=initial_lr)\n",
    "# decoder_optimizer = optim.SGD(decoder.parameters(), lr=initial_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.720215797424316"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_iter(ingredients, recipes, ing_lens, rec_lens, encoder, decoder_attn, \n",
    "#            encoder_optimizer, decoder_optimizer, criterion=nn.NLLLoss(),\n",
    "#            decoder_mode=\"attention\", vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_recipe_ds = RecipeDataset(dev_df, vocab, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataloader = DataLoader(dev_recipe_ds, batch_size=4, shuffle=False, \n",
    "                            collate_fn=pad_collate(vocab, train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.eval()\n",
    "# decoder.eval()\n",
    "# with torch.no_grad():\n",
    "#     all_decoder_outs = get_predictions_iter(ingredients_padded, ingr_lens,\n",
    "#                                         encoder, decoder, vocab, max_recipe_len=MAX_RECIPE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/194 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194/194 [01:39<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "all_decoder_outs, all_gt_recipes = eval(encoder, decoder, dev_recipe_ds, vocab,\n",
    "                                        max_recipe_len=MAX_RECIPE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/envs/nlp/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/student/anaconda3/envs/nlp/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.4558993159943767e-156"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_bleu(all_gt_recipes, all_decoder_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 775/775 [00:44<00:00, 17.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0491881954127885"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_meteor(all_gt_recipes, all_decoder_outs, split_gt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ings = get_all_ingredients(\"./ingredient_set.json\")\n",
    "all_ings_regex = get_ingredients_regex(all_ings)\n",
    "metric_sample_ings, metric_sample_gold_recipe, metric_sample_generated_recipe = \\\n",
    "    load_metric_sample(\"./metric_sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Input ingredients in text=====\n",
      "['orange juice', 'strawberries', 'lemon juice', 'sugar', 'water']\n",
      "\n",
      "=====All ingredients in text===== \n",
      "['vanilla ice cream', 'orange juice', 'strawberries', 'cantaloupe', 'lemon juice', 'sugar', 'water']\n",
      "\n",
      "proportion of input ingredients: 1.0\n",
      "number of extra ingredients: 2\n"
     ]
    }
   ],
   "source": [
    "prop_inp_ings, n_extra_ings = get_prop_input_num_extra_ingredients(\n",
    "    metric_sample_ings, metric_sample_generated_recipe, all_ings_regex, verbose=True,\n",
    "    metric_sample=True)\n",
    "print(f\"\\nproportion of input ingredients: {prop_inp_ings}\\nnumber of extra ingredients: {n_extra_ings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.14346607531819988, METEOR score: 0.5736654804270463\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calc_bleu([metric_sample_gold_recipe], [metric_sample_generated_recipe], split_gen=True)\n",
    "meteor_score = calc_meteor([metric_sample_gold_recipe], [metric_sample_generated_recipe], split_gen=True)\n",
    "print(f\"BLEU score: {bleu_score}, METEOR score: {meteor_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
