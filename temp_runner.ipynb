{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from data import *\n",
    "from encoder_decoder import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 31989101\n",
    "HIDDEN_SIZE = 256\n",
    "MAX_INGR_LEN = 150\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## ensuring reproducibility\n",
    "def reset_rng():\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "reset_rng()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to easily read ingredients and instructions\n",
    "pd.set_option('display.max_colwidth', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./Cooking_Dataset\"\n",
    "\n",
    "train_df_orig = pd.read_csv(os.path.join(data_root, \"train.csv\"), usecols=['Ingredients', 'Recipe'])\n",
    "dev_df_orig = pd.read_csv(os.path.join(data_root, \"dev.csv\"), usecols=['Ingredients', 'Recipe'])\n",
    "test_df_orig = pd.read_csv(os.path.join(data_root, \"test.csv\"), usecols=['Ingredients', 'Recipe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples before preprocessing: 101340\n",
      "Number of data samples after preprocessing: 99036 (97.726%)\n"
     ]
    }
   ],
   "source": [
    "train_df = preprocess_data(train_df_orig, max_ingr_len=MAX_INGR_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99036/99036 [00:08<00:00, 12164.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44683"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.populate(train_df)\n",
    "vocab.n_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_ds = RecipeDataset(train_df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(vocab.n_unique_words, hidden_size=HIDDEN_SIZE, padding_value=vocab.word2index[PAD_WORD]).to(DEVICE)\n",
    "# in the training script, decoder is always fed a non-end token and thus never needs to generate padding\n",
    "decoder = DecoderRNN(hidden_size=HIDDEN_SIZE, output_size=vocab.n_unique_words-1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/20, enc lr scheduler: [0.5], dec lr scheduler: [0.5]\n",
      "(Epoch 0, iter 10/1548) Average loss so far: 10.664\n",
      "(Epoch 0, iter 20/1548) Average loss so far: 10.558\n",
      "(Epoch 0, iter 30/1548) Average loss so far: 10.454\n",
      "(Epoch 0, iter 40/1548) Average loss so far: 10.340\n",
      "(Epoch 0, iter 50/1548) Average loss so far: 10.211\n",
      "(Epoch 0, iter 60/1548) Average loss so far: 10.065\n",
      "(Epoch 0, iter 70/1548) Average loss so far: 9.889\n",
      "(Epoch 0, iter 80/1548) Average loss so far: 9.662\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/temp_runner.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/student/HDD%202/Michelle/NLP/neural-recipe-generator/temp_runner.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m enc_scheduler \u001b[39m=\u001b[39m CosineAnnealingLR(encoder_optimizer, T_max\u001b[39m=\u001b[39mn_epochs, eta_min\u001b[39m=\u001b[39mmin_lr)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/student/HDD%202/Michelle/NLP/neural-recipe-generator/temp_runner.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dec_scheduler \u001b[39m=\u001b[39m CosineAnnealingLR(decoder_optimizer, T_max\u001b[39m=\u001b[39mn_epochs, eta_min\u001b[39m=\u001b[39mmin_lr)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/student/HDD%202/Michelle/NLP/neural-recipe-generator/temp_runner.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m epoch_losses \u001b[39m=\u001b[39m train(encoder, decoder, encoder_optimizer, decoder_optimizer, recipe_ds, \n\u001b[1;32m     <a href='vscode-notebook-cell:/media/student/HDD%202/Michelle/NLP/neural-recipe-generator/temp_runner.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                      n_epochs\u001b[39m=\u001b[39;49mn_epochs, vocab\u001b[39m=\u001b[39;49mvocab, batch_size\u001b[39m=\u001b[39;49mbatch_size, \n\u001b[1;32m     <a href='vscode-notebook-cell:/media/student/HDD%202/Michelle/NLP/neural-recipe-generator/temp_runner.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                      enc_lr_scheduler\u001b[39m=\u001b[39;49menc_scheduler, dec_lr_scheduler\u001b[39m=\u001b[39;49mdec_scheduler, \n\u001b[1;32m     <a href='vscode-notebook-cell:/media/student/HDD%202/Michelle/NLP/neural-recipe-generator/temp_runner.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                      verbose_iter_interval\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/train.py:120\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, encoder_optimizer, decoder_optimizer, dataset, n_epochs, vocab, batch_size, enc_lr_scheduler, dec_lr_scheduler, verbose, verbose_iter_interval)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, iter \u001b[39m\u001b[39m{\u001b[39;00miter_idx\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mtotal_iters\u001b[39m}\u001b[39;00m\u001b[39m) Average loss so far: \u001b[39m\u001b[39m{\u001b[39;00mprint_epoch_loss\u001b[39m/\u001b[39mverbose_iter_interval\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m     print_epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 120\u001b[0m loss \u001b[39m=\u001b[39m train_iter(ingredients, recipes, ing_lens, rec_lens, encoder, decoder, \n\u001b[1;32m    121\u001b[0m                        encoder_optimizer, decoder_optimizer, criterion, \n\u001b[1;32m    122\u001b[0m                        vocab\u001b[39m=\u001b[39;49mvocab \u001b[39m# remove later\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m                        )\n\u001b[1;32m    124\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m    125\u001b[0m print_epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "File \u001b[0;32m/media/student/HDD 2/Michelle/NLP/neural-recipe-generator/train.py:92\u001b[0m, in \u001b[0;36mtrain_iter\u001b[0;34m(ingredients, recipes, ing_lens, rec_lens, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, vocab)\u001b[0m\n\u001b[1;32m     89\u001b[0m loss \u001b[39m=\u001b[39m criterion(all_decoder_outs, all_gt)\n\u001b[1;32m     91\u001b[0m \u001b[39m## backpropagation\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     94\u001b[0m \u001b[39m## update params\u001b[39;00m\n\u001b[1;32m     95\u001b[0m encoder_optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_lr=0.8\n",
    "min_lr = 0.01\n",
    "n_epochs = 20\n",
    "batch_size=128\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=initial_lr)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=initial_lr)\n",
    "enc_scheduler = CosineAnnealingLR(encoder_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "dec_scheduler = CosineAnnealingLR(decoder_optimizer, T_max=n_epochs, eta_min=min_lr)\n",
    "epoch_losses = train(encoder, decoder, encoder_optimizer, decoder_optimizer, recipe_ds, \n",
    "                     n_epochs=n_epochs, vocab=vocab, batch_size=batch_size, \n",
    "                     enc_lr_scheduler=enc_scheduler, dec_lr_scheduler=dec_scheduler, \n",
    "                     verbose_iter_interval=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
