{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand([2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_for_k(x, k, dim=1):\n",
    "    dims = [1]*x.ndim\n",
    "    dims[dim] = k\n",
    "    return x.repeat(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_negative_constraint(num_ks, curr_decoder_outs_shape, all_decoder_outs, neg_constraints):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        num_ks (int): number of hypothesis which have not ended\n",
    "        curr_decoder_outs_shape: shape of current decoder output\n",
    "        all_decoder_outs (_type_): [k, |generation|]\n",
    "        neg_constraints (List[List[int]]): list of negative constraints, nested list because constraints can be multi-word\n",
    "    \"\"\"\n",
    "\n",
    "    # >1 if neg constraint satisfied, 0 otherwise\n",
    "    # >1 makes sense if multiple negative constraints are satisfied (yielding larger penalty)\n",
    "    neg_constraint_satisfied = torch.zeros(curr_decoder_outs_shape, device=DEVICE)\n",
    "\n",
    "    for ki in range(num_ks):\n",
    "        for neg_cons in neg_constraints:\n",
    "            neg_constraint_exists = True\n",
    "            neg_idx = neg_cons[-1]\n",
    "\n",
    "            # to check if a negative constraint is satisfied (irreversible unsatisfaction) \n",
    "            # so need to check from back to front (only relevant for multi-word constraints)\n",
    "            for word_idx, constraint_word in enumerate(neg_cons[:-1][::-1]):\n",
    "                # if mismatch, then neg constraint is not satisfied\n",
    "                if all_decoder_outs[ki][-(word_idx+1)] != constraint_word:\n",
    "                    neg_constraint_exists = False\n",
    "                    break\n",
    "\n",
    "            neg_constraint_satisfied[ki][neg_idx] += neg_constraint_exists\n",
    "    return neg_constraint_satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_low_likelihood(alpha, likelihood):\n",
    "    \"\"\"Detect likelihoods < top-alpha\n",
    "\n",
    "    Args:\n",
    "        alpha (_type_): _description_\n",
    "        likelihood (_type_): [k, |V|-1]\n",
    "    \"\"\"\n",
    "    # get the minimum value to be included within the top-alpha\n",
    "    likelihood_penalty_thresh = likelihood.flatten().topk(alpha).values.min()\n",
    "\n",
    "    return likelihood < likelihood_penalty_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_irreversible_satisfaction(num_ks, k_irreversible_satisfaction, all_decoder_outs, pos_constraints,\n",
    "                                   out_size):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        num_ks (int): number of hypothesis which have not ended\n",
    "        beta (_type_): top-beta number of irreversible satisfactions to keep\n",
    "        k_irreversible_satisfaction (_type_): number of irreversible satisfactions per hypothesis; [k]\n",
    "        all_decoder_outs (_type_): list of previous generations for each hypothesis [k, |generation|]\n",
    "        pos_constraints (List[List[List[int]]]): 3D list of shape [max_k, num positive constraints, length of positive constraint]\n",
    "        out_size: dimension of generations |Vocab|-1\n",
    "    \"\"\"\n",
    "    # [k, |V|-1]\n",
    "    k_irreversible_satisfaction_now = k_irreversible_satisfaction[:, None].repeat(1, out_size)\n",
    "    pos_constraints_satisfied = torch.full_like(k_irreversible_satisfaction_now, -1)\n",
    "\n",
    "    for ki in range(num_ks):\n",
    "        for pos_cons_idx, pos_cons in enumerate(pos_constraints[ki]):\n",
    "            pos_constraint_exist = True\n",
    "            \n",
    "            ## similar to detecting irreversible unsatisfaction, \n",
    "            # we check from last (current) to first (previous generated text)\n",
    "            pos_idx = pos_cons[-1]\n",
    "\n",
    "            # checkds from 2nd last to first (only relevant for multi-word constraints)\n",
    "            for word_idx, constraint_word in enumerate(pos_cons[:-1][::-1]): \n",
    "                if all_decoder_outs[ki][-(word_idx+1)] != constraint_word:\n",
    "                    pos_constraint_exist = False\n",
    "                    break\n",
    "            k_irreversible_satisfaction_now[ki][pos_idx] += pos_constraint_exist\n",
    "            pos_constraints_satisfied[ki][pos_idx] = pos_cons_idx\n",
    "    return k_irreversible_satisfaction_now, pos_constraints_satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_low_irreversible_satisfactions(k_irreversible_satisfactions_now, # [k, |V|-1]\n",
    "                                          beta):\n",
    "    # get the minimum number of satisfied clauses to be included within the top-beta\n",
    "    # need to use unique because many candidates can have the same number of satisfied clauses\n",
    "    satisfaction_penalty_thresh = k_irreversible_satisfactions_now.flatten().unique()[-beta].item()\n",
    "    return k_irreversible_satisfactions_now < satisfaction_penalty_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportion_completion_reward(num_ks,\n",
    "                                     scores, # [k, |V|-1]\n",
    "                                     pos_constraints, # List[List[List[int]]]; [k, num constraints, len constraint]\n",
    "                                     all_decoder_outs # List [k, |generation|]\n",
    "                                     ):\n",
    "    reward = torch.zeros_like(scores)\n",
    "\n",
    "    for ki in range(num_ks):\n",
    "        for pos_cons in pos_constraints[ki]:\n",
    "            ## just like in the paper, we also reward partial completion (reversible satisfaction)\n",
    "            # to do this we need to do constraint prefix comparison for lengths i=0...|constraint|\n",
    "            # because if a constraint is: [0, 1, 2, 3], a partial completion could be [0], [0,1], [0,1,2]\n",
    "            # with full completion: [0,1,2,3]\n",
    "            for word_idx, constraint_word in enumerate(pos_cons):\n",
    "                if word_idx == 0 or all_decoder_outs[ki][-word_idx:] == pos_cons[:word_idx]:\n",
    "                    reward[ki][constraint_word] += (word_idx+1) / len(pos_cons)\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_neuro_decoder_iter(decoder, decoder_hidden, decoder_cell, encoder_houts,\n",
    "                            ingredients, max_recipe_len, vocab,\n",
    "                            pos_constraints, neg_constraints, k, alpha, beta,\n",
    "                            neg_constraint_penalty, likelihood_penalty, low_irr_satisfaction_penalty, lam,\n",
    "                            decoder_mode=\"basic\"):\n",
    "    \"\"\"Neurological decoding for a particular sample in batch.\n",
    "\n",
    "    Args:\n",
    "        decoder (_type_): _description_\n",
    "        decoder_hidden (_type_): [1, N=1, H]\n",
    "        decoder_cell (_type_): [1, N=1, H]\n",
    "        encoder_houts (_type_): [N=1, L_i, H]\n",
    "        ingredients (_type_): [N=1, L_i]\n",
    "        max_recipe_len (_type_): _description_\n",
    "        vocab (_type_): _description_\n",
    "        pos_constraints (List[List[int]]): list of positive constraints, nested list because constraints can be multi-word \n",
    "                                           IMPORTANT: these are expected to be transformed to index using vocab\n",
    "        neg_constraints (List[List[int]]): list of negative constraints, nested list because constraints can be multi-word\n",
    "                                           IMPORTANT: these are expected to be transformed to index using vocab\n",
    "        k (_type_): number of hypothesis per sample\n",
    "        alpha (_type_): top-alpha likelihood which are not pruned\n",
    "        beta (_type_): top-beta number of satisfied clauses which are not pruned\n",
    "        neg_constraint_penalty (_type_): penalty for including negative constraint\n",
    "        likelihood_penalty (_type_): penalty for not being in top-alpha likelihood\n",
    "        low_irr_satisfaction_penalty (_type_): penalty for not being in top-beta no. of satisfied clauses\n",
    "        lam (_type_): lambda to add constraint progress to score\n",
    "        decoder_mode (str, optional): _description_. Defaults to \"basic\".\n",
    "    \"\"\"\n",
    "    assert decoder_mode == \"attention\", \"best model is attention, should be using attention!\"\n",
    "\n",
    "    K = torch.arange(k)\n",
    "    \n",
    "    all_decoder_outs = [[REC_START] for _ in range(k)] # stores decoder outputs for each hypothesis\n",
    "\n",
    "    decoder_input = torch.full([k], SPECIAL_TAGS[REC_START], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # stores the running likelihoods for the k hypotheses\n",
    "    k_likelihoods = torch.ones_like(decoder_input) # [k]\n",
    "\n",
    "    # number of irreversible satisfactions so far for each hypothesis\n",
    "    k_irreversible_satisfaction = torch.zeros_like(decoder_input) # [k]\n",
    "\n",
    "    # lists *remaining* positive constraints for each hypotheses\n",
    "    # once a positive constraint has been fully satisfied (irreversible satisfaction), it is removed\n",
    "    # 3D list of shape [max_k, num positive constraints, length of positive constraint]\n",
    "    pos_constraints_i = [pos_constraints for _ in range(k)]\n",
    "\n",
    "    ## initialize inputs as the same for all ks because all of them have the same ingredients\n",
    "    encoder_houts_i = repeat_for_k(encoder_houts, k, dim=0) # [N=max_K, L_i, H]\n",
    "    decoder_hidden_i = repeat_for_k(decoder_hidden, num_ks, dim=1) # [1, N=max_K, H]\n",
    "    decoder_cell_i = repeat_for_k(decoder_cell, num_ks, dim=1) # [1, N=max_K, H]\n",
    "    ingredients_i = repeat_for_k(ingredients, k, dim=0) # [N=max_K, L_i]\n",
    "\n",
    "    for i in range(max_recipe_len - 1): # generations are bounded by max length (-1 because of EOS)\n",
    "        ## precondition: K is the list of hypotheses which have not ended\n",
    "\n",
    "        num_ks = len(K) # some hypotheses can finish early so need to udpate this every iter\n",
    "\n",
    "        ## attention\n",
    "        # decoder_out: log probs [k, |Vocab|-1]\n",
    "        decoder_out, decoder_hidden_i, decoder_cell_i, attn_weights_i = decoder(\n",
    "            decoder_input, decoder_hidden_i, decoder_cell_i, encoder_houts_i, ingredients_i\n",
    "        )\n",
    "\n",
    "        # multiply all log probs with running log probs\n",
    "        # [k, |V|-1] * [k, 1] = [k, |V|-1]\n",
    "        likelihood_i = decoder_out * k_likelihoods[K].unsqueeze(-1)\n",
    "\n",
    "        scores = likelihood_i.clone() # used for selection\n",
    "\n",
    "        ############# PRUNING #############\n",
    "\n",
    "        # detect generations which will cause irreversible unsatisfaction; [k, |V|-1]\n",
    "        neg_constraint_satisfied = detect_negative_constraint(\n",
    "            num_ks, likelihood_i.shape, all_decoder_outs[K], neg_constraints)\n",
    "        \n",
    "        # detect generations with low likelihood; [k, |V|-1]\n",
    "        low_likelihoods = detect_low_likelihood(alpha, likelihood_i)\n",
    "\n",
    "        # get potential total irreversible satisfaction (including already satisfied clauses) for each candidate\n",
    "        # pos_constraints_satisfied: [k, |V|-1]\n",
    "        k_irreversible_satisfaction[K], pos_constraints_satisfied = update_irreversible_satisfaction(\n",
    "            num_ks, k_irreversible_satisfaction[K], all_decoder_outs[K], pos_constraints_i, \n",
    "            out_size=likelihood_i.size(-1))\n",
    "        \n",
    "        # detect generations with < top-beta number of irreversibly satisfied clauses; [k, |V|-1]\n",
    "        low_irreversible_satisfaction = detect_low_irreversible_satisfactions(\n",
    "            k_irreversible_satisfaction[K], beta)\n",
    "        \n",
    "        # perform soft pruning, i.e. penalizing instead of filtering out (see report)\n",
    "        penalties = neg_constraint_satisfied * neg_constraint_penalty + \\\n",
    "                    low_likelihoods * likelihood_penalty + \\\n",
    "                    low_irreversible_satisfaction * low_irr_satisfaction_penalty\n",
    "        \n",
    "        scores -= penalties\n",
    "        \n",
    "        ############# SELECTION #############\n",
    "\n",
    "        # get rewards for partial/full completion\n",
    "        rewards = get_proportion_completion_reward(num_ks, scores, pos_constraints_i, all_decoder_outs[K])\n",
    "\n",
    "        scores += rewards # [k, |V|-1]\n",
    "\n",
    "        # select top-k based on scores across all candidates\n",
    "        topk_scores, topk_inds = scores.flatten().topk(num_ks)\n",
    "        k_origin = torch.div(topk_inds, scores.size(-1), rounding_mode=\"floor\")\n",
    "        k_origin_global = K[k_origin] # get global k\n",
    "        word_idx = topk_inds % scores.size(-1) # k top words\n",
    "\n",
    "        decoder_input = word_idx # [k]\n",
    "        for k_glob in k_origin_global.tolist():\n",
    "            all_decoder_outs[k_glob].append(word_idx)\n",
    "\n",
    "        k_likelihoods = likelihood_i[k_origin, word_idx] # [k]\n",
    "        k_irreversible_satisfaction = k_irreversible_satisfaction[k_origin_global] # [k]\n",
    "\n",
    "        pos_constraints_chosen = [pos_constraints_i[ki] for ki in k_origin]\n",
    "\n",
    "        for i, (ki, wi) in enumerate(zip(k_origin, word_idx)):\n",
    "            # remove irreversibly satisfied constraint\n",
    "            pos_constraints_chosen[i] = [c for cidx, c in enumerate(pos_constraints_chosen[i])\n",
    "                                         if cidx != pos_constraints_satisfied[ki, wi]]\n",
    "        pos_constraints_i = pos_constraints_chosen\n",
    "\n",
    "        ############# PREPARE FOR NEXT ITERATION #############\n",
    "\n",
    "        ## check if any of the hypotheses has ended (update K)\n",
    "        not_eor = word_idx != SPECIAL_TAGS[REC_END]\n",
    "        K=K[not_eor]\n",
    "\n",
    "        if len(K) < 1:\n",
    "            break\n",
    "\n",
    "        ## postcondition: K is the list of hypotheses which have not ended\n",
    "\n",
    "        ## determine inputs for next iter\n",
    "\n",
    "        ## all the same so no need to index using K\n",
    "        encoder_houts_i = encoder_houts[:len(K)] # [N=k, L_i, H]\n",
    "        ingredients_i = ingredients_i[:len(K)]\n",
    "\n",
    "        ## different based on k\n",
    "        decoder_hidden_i = decoder_hidden_i[k_origin][not_eor]\n",
    "        decoder_cell_i = decoder_cell_i[k_origin][not_eor]\n",
    "        pos_constraints_i = [pos_constraints_i[i] for i in not_eor.nonzero().flatten()]\n",
    "    else:\n",
    "        for ki in K.tolist():\n",
    "            all_decoder_outs[ki].append(REC_END)\n",
    "\n",
    "    return all_decoder_outs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
