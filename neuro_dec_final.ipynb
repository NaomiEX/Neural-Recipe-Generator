{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "from constants import *\n",
    "from encoder_decoder import *\n",
    "SEED = 0\n",
    "## ensuring reproducibility\n",
    "def reset_rng():\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "reset_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand([2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_for_k(x, k, dim=1):\n",
    "    dims = [1]*x.ndim\n",
    "    dims[dim] = k\n",
    "    return x.repeat(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_negative_constraint(num_ks, curr_decoder_outs_shape, all_decoder_outs, neg_constraints):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        num_ks (int): number of hypothesis which have not ended\n",
    "        curr_decoder_outs_shape: shape of current decoder output [k, |V|-1]\n",
    "        all_decoder_outs (list): [k, |generation|]\n",
    "        neg_constraints (List[List[int]]): list of negative constraints, nested list because constraints can be multi-word\n",
    "    \"\"\"\n",
    "\n",
    "    # >1 if neg constraint satisfied, 0 otherwise\n",
    "    # >1 makes sense if multiple negative constraints are satisfied (yielding larger penalty)\n",
    "    neg_constraint_satisfied = torch.zeros(curr_decoder_outs_shape, device=DEVICE) # [k, |V|-1]\n",
    "\n",
    "    for ki in range(num_ks):\n",
    "        for neg_cons in neg_constraints:\n",
    "            neg_constraint_exists = True\n",
    "            neg_idx = neg_cons[-1]\n",
    "\n",
    "            if (len(neg_cons) - 1) > len(all_decoder_outs[ki]):\n",
    "                continue # i.e. neg_constraint_exists=False\n",
    "\n",
    "            # to check if a negative constraint is satisfied (irreversible unsatisfaction) \n",
    "            # so need to check from back to front (only relevant for multi-word constraints)\n",
    "            for word_idx, constraint_word in enumerate(neg_cons[:-1][::-1]):\n",
    "                # if mismatch, then neg constraint is not satisfied\n",
    "                if all_decoder_outs[ki][-(word_idx+1)] != constraint_word:\n",
    "                    neg_constraint_exists = False\n",
    "                    break\n",
    "\n",
    "            neg_constraint_satisfied[ki][neg_idx] += neg_constraint_exists\n",
    "    return neg_constraint_satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_low_likelihood(alpha, likelihood):\n",
    "    \"\"\"Detect likelihoods < top-alpha\n",
    "\n",
    "    Args:\n",
    "        alpha (_type_): _description_\n",
    "        likelihood (_type_): [k, |V|-1]\n",
    "    \"\"\"\n",
    "    # get the minimum value to be included within the top-alpha\n",
    "    likelihood_penalty_thresh = likelihood.flatten().topk(alpha).values.min()\n",
    "\n",
    "    return likelihood < likelihood_penalty_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_irreversible_satisfaction(num_ks, k_irreversible_satisfaction, all_decoder_outs, pos_constraints,\n",
    "                                   out_size):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        num_ks (int): number of hypothesis which have not ended\n",
    "        beta (_type_): top-beta number of irreversible satisfactions to keep\n",
    "        k_irreversible_satisfaction (_type_): number of irreversible satisfactions per hypothesis; [k]\n",
    "        all_decoder_outs (_type_): list of previous generations for each hypothesis [k, |generation|]\n",
    "        pos_constraints (List[List[List[int]]]): 3D list of shape [max_k, num positive constraints, length of positive constraint]\n",
    "        out_size: dimension of generations |Vocab|-1\n",
    "    \"\"\"\n",
    "    # [k, |V|-1]\n",
    "    k_irreversible_satisfaction_now = k_irreversible_satisfaction[:, None].repeat(1, out_size)\n",
    "    pos_constraints_satisfied = torch.full_like(k_irreversible_satisfaction_now, -1)\n",
    "\n",
    "    for ki in range(num_ks):\n",
    "        for pos_cons_idx, pos_cons in enumerate(pos_constraints[ki]):\n",
    "            pos_constraint_exist = True\n",
    "            \n",
    "            ## similar to detecting irreversible unsatisfaction, \n",
    "            # we check from last (current) to first (previous generated text)\n",
    "            pos_idx = pos_cons[-1]\n",
    "\n",
    "            if (len(pos_cons) - 1) > len(all_decoder_outs[ki]):\n",
    "                continue # i.e. pos_constraint_exist=False\n",
    "\n",
    "            # checkds from 2nd last to first (only relevant for multi-word constraints)\n",
    "            for word_idx, constraint_word in enumerate(pos_cons[:-1][::-1]): \n",
    "                if all_decoder_outs[ki][-(word_idx+1)] != constraint_word:\n",
    "                    pos_constraint_exist = False\n",
    "                    break\n",
    "\n",
    "            if pos_constraint_exist:\n",
    "                k_irreversible_satisfaction_now[ki][pos_idx] += 1\n",
    "                pos_constraints_satisfied[ki][pos_idx] = pos_cons_idx\n",
    "                \n",
    "    return k_irreversible_satisfaction_now, pos_constraints_satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_low_irreversible_satisfactions(k_irreversible_satisfactions_now, # [k, |V|-1]\n",
    "                                          beta):\n",
    "    # get the minimum number of satisfied clauses to be included within the top-beta\n",
    "    # need to use unique because many candidates can have the same number of satisfied clauses\n",
    "    unique_num_irreversible_satisfactions = k_irreversible_satisfactions_now.flatten().unique()\n",
    "    satisfaction_penalty_thresh = unique_num_irreversible_satisfactions[-min(beta, len(unique_num_irreversible_satisfactions))].item()\n",
    "    return k_irreversible_satisfactions_now < satisfaction_penalty_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proportion_completion_reward(num_ks,\n",
    "                                     scores, # [k, |V|-1]\n",
    "                                     pos_constraints, # List[List[List[int]]]; [k, num constraints, len constraint]\n",
    "                                     all_decoder_outs, # List [k, |generation|]\n",
    "                                     lam=0.5\n",
    "                                     ):\n",
    "    reward = torch.zeros_like(scores)\n",
    "\n",
    "    for ki in range(num_ks):\n",
    "        for pos_cons in pos_constraints[ki]:\n",
    "            ## just like in the paper, we also reward partial completion (reversible satisfaction)\n",
    "            # to do this we need to do constraint prefix comparison for lengths i=0...|constraint|\n",
    "            # because if a constraint is: [0, 1, 2, 3], a partial completion could be [0], [0,1], [0,1,2]\n",
    "            # with full completion: [0,1,2,3]\n",
    "            for word_idx, constraint_word in enumerate(pos_cons):\n",
    "                if word_idx > len(all_decoder_outs[ki]):\n",
    "                    break\n",
    "                if word_idx == 0 or all_decoder_outs[ki][-word_idx:] == pos_cons[:word_idx]:\n",
    "                    reward[ki][constraint_word] = max((word_idx+1) / len(pos_cons), reward[ki][constraint_word])\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "    return lam * reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_neuro_decoder_iter(decoder, decoder_hidden, decoder_cell, encoder_houts,\n",
    "                            ingredients, max_recipe_len,\n",
    "                            pos_constraints, neg_constraints, k, alpha, beta,\n",
    "                            neg_constraint_penalty, likelihood_penalty, low_irr_satisfaction_penalty, lam,\n",
    "                            decoder_mode=\"attention\"):\n",
    "    \"\"\"Neurological decoding for a particular sample in batch.\n",
    "\n",
    "    Args:\n",
    "        decoder (_type_): _description_\n",
    "        decoder_hidden (_type_): [1, N=1, H]\n",
    "        decoder_cell (_type_): [1, N=1, H]\n",
    "        encoder_houts (_type_): [N=1, L_i, H]\n",
    "        ingredients (_type_): [N=1, L_i]\n",
    "        max_recipe_len (_type_): _description_\n",
    "        pos_constraints (List[List[int]]): list of positive constraints, nested list because constraints can be multi-word \n",
    "                                           IMPORTANT: these are expected to be transformed to index using vocab\n",
    "        neg_constraints (List[List[int]]): list of negative constraints, nested list because constraints can be multi-word\n",
    "                                           IMPORTANT: these are expected to be transformed to index using vocab\n",
    "        k (_type_): number of hypothesis per sample\n",
    "        alpha (_type_): top-alpha likelihood which are not pruned\n",
    "        beta (_type_): top-beta number of satisfied clauses which are not pruned\n",
    "        neg_constraint_penalty (_type_): penalty for including negative constraint\n",
    "        likelihood_penalty (_type_): penalty for not being in top-alpha likelihood\n",
    "        low_irr_satisfaction_penalty (_type_): penalty for not being in top-beta no. of satisfied clauses\n",
    "        lam (_type_): lambda to add constraint progress to score\n",
    "        decoder_mode (str, optional): _description_. Defaults to \"basic\".\n",
    "    \"\"\"\n",
    "    assert decoder_mode == \"attention\", \"best model is attention, should be using attention!\"\n",
    "\n",
    "    K = torch.tensor([0]) # start with 1 hypothesis\n",
    "    \n",
    "    all_decoder_outs = [[SPECIAL_TAGS[REC_START]] for _ in range(k)] # stores decoder outputs for each hypothesis; [max_K]\n",
    "\n",
    "    decoder_input = torch.full([k], SPECIAL_TAGS[REC_START], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # stores the running likelihoods for the k hypotheses\n",
    "    k_likelihoods = torch.ones_like(decoder_input) # [max_K]\n",
    "\n",
    "    # number of irreversible satisfactions so far for each hypothesis\n",
    "    k_irreversible_satisfaction = torch.zeros_like(decoder_input) # [k]\n",
    "\n",
    "    # lists *remaining* positive constraints for each hypotheses\n",
    "    # once a positive constraint has been fully satisfied (irreversible satisfaction), it is removed\n",
    "    # 3D list of shape [max_k, num positive constraints, length of positive constraint]\n",
    "    # pos_constraints_i = [pos_constraints for _ in range(k)]\n",
    "    pos_constraints_i = [pos_constraints]\n",
    "\n",
    "    ## initialize inputs as the same for all ks because all of them have the same ingredients\n",
    "    # encoder_houts_i = repeat_for_k(encoder_houts, k, dim=0) # [N=max_K, L_i, H]\n",
    "    # decoder_hidden_i = repeat_for_k(decoder_hidden, k, dim=1) # [1, N=max_K, H]\n",
    "    # decoder_cell_i = repeat_for_k(decoder_cell, k, dim=1) # [1, N=max_K, H]\n",
    "    # ingredients_i = repeat_for_k(ingredients, k, dim=0) # [N=max_K, L_i]\n",
    "    encoder_houts_i = encoder_houts\n",
    "    decoder_hidden_i = decoder_hidden\n",
    "    decoder_cell_i = decoder_cell\n",
    "    ingredients_i = ingredients\n",
    "\n",
    "    for recipe_i in range(max_recipe_len - 1): # generations are bounded by max length (-1 because of EOS)\n",
    "        ## precondition: K is the list of hypotheses which have not ended\n",
    "\n",
    "        num_ks = len(K) # some hypotheses can finish early so need to udpate this every iter\n",
    "        valid_all_decoder_outs = [all_decoder_outs[i] for i in K]\n",
    "\n",
    "        ## attention\n",
    "        # decoder_out: log probs [k, |Vocab|-1]\n",
    "        decoder_out, decoder_hidden_i, decoder_cell_i, attn_weights_i = decoder(\n",
    "            decoder_input[K], decoder_hidden_i, decoder_cell_i, encoder_houts_i, ingredients_i\n",
    "        )\n",
    "\n",
    "        # sum all log probs with running log probs\n",
    "        # [k, |V|-1] + [k, 1] = [k, |V|-1]\n",
    "        likelihood_i = decoder_out + k_likelihoods[K].unsqueeze(-1)\n",
    "\n",
    "        scores = likelihood_i.clone() # used for selection (can no longer be interpreted as probabilities so we preserve likelihoods)\n",
    "\n",
    "        ############# PRUNING #############\n",
    "\n",
    "        # detect generations which will cause irreversible unsatisfaction; [k, |V|-1]\n",
    "        neg_constraint_satisfied = detect_negative_constraint(\n",
    "            num_ks, likelihood_i.shape, valid_all_decoder_outs, neg_constraints)\n",
    "        \n",
    "        # detect generations with low likelihood; [k, |V|-1]\n",
    "        low_likelihoods = detect_low_likelihood(alpha, likelihood_i)\n",
    "\n",
    "        # get potential total irreversible satisfaction (including already satisfied clauses) for each candidate\n",
    "        # k_irreversible_satisfaction_now: [k, |V|-1]\n",
    "        # pos_constraints_satisfied: [k, |V|-1]\n",
    "        k_irreversible_satisfaction_now, pos_constraints_satisfied = update_irreversible_satisfaction(\n",
    "            num_ks, k_irreversible_satisfaction[K], valid_all_decoder_outs, pos_constraints_i, \n",
    "            out_size=likelihood_i.size(-1))\n",
    "        \n",
    "        # detect generations with < top-beta number of irreversibly satisfied clauses; [k, |V|-1]\n",
    "        low_irreversible_satisfaction = detect_low_irreversible_satisfactions(\n",
    "            k_irreversible_satisfaction_now, beta)\n",
    "        \n",
    "        # perform soft pruning, i.e. penalizing instead of filtering out (see report)\n",
    "        penalties = neg_constraint_satisfied * neg_constraint_penalty + \\\n",
    "                    low_likelihoods * likelihood_penalty + \\\n",
    "                    low_irreversible_satisfaction * low_irr_satisfaction_penalty\n",
    "        \n",
    "        scores -= penalties\n",
    "        \n",
    "        ############# SELECTION #############\n",
    "\n",
    "        # get rewards for partial/full completion\n",
    "        rewards = get_proportion_completion_reward(num_ks, scores, pos_constraints_i, valid_all_decoder_outs)\n",
    "\n",
    "        scores += rewards # [k, |V|-1]\n",
    "\n",
    "        # select top-k based on scores across all candidates\n",
    "        topk_scores, topk_inds = scores.flatten().topk(num_ks if recipe_i > 0 else k)\n",
    "        k_origin = torch.div(topk_inds, scores.size(-1), rounding_mode=\"floor\")\n",
    "        k_origin_global = K[k_origin] # get global k\n",
    "        word_idx = topk_inds % scores.size(-1) # k top words\n",
    "\n",
    "        decoder_input = word_idx # [k]\n",
    "        prev_all_decoder_outs = deepcopy(all_decoder_outs)\n",
    "        for ki, k_glob in zip((K if recipe_i > 0 else range(3)), k_origin_global):\n",
    "            all_decoder_outs[ki] = prev_all_decoder_outs[k_glob] + [word_idx[ki].item()]\n",
    "            # all_decoder_outs[k_glob].append(word_idx[k_glob].item())\n",
    "\n",
    "        k_likelihoods[K] = likelihood_i[k_origin, word_idx] # [k]\n",
    "        k_irreversible_satisfaction = k_irreversible_satisfaction_now[k_origin, word_idx] # [k]\n",
    "\n",
    "        pos_constraints_chosen = [pos_constraints_i[ki] for ki in k_origin]\n",
    "\n",
    "        for i, (ki, wi) in enumerate(zip(k_origin, word_idx)):\n",
    "            # remove irreversibly satisfied constraint\n",
    "            pos_constraints_chosen[i] = [c for cidx, c in enumerate(pos_constraints_chosen[i])\n",
    "                                         if cidx != pos_constraints_satisfied[ki, wi]]\n",
    "        pos_constraints_i = pos_constraints_chosen\n",
    "\n",
    "        ############# PREPARE FOR NEXT ITERATION #############\n",
    "\n",
    "        ## check if any of the hypotheses has ended (update K)\n",
    "        not_eor = word_idx != SPECIAL_TAGS[REC_END]\n",
    "        K = torch.arange(k)[K][not_eor] if recipe_i > 0 else torch.arange(k)[not_eor]\n",
    "        # K=K[not_eor] if recipe_i > 0 else torch.arange(k)[not_eor]\n",
    "        # K=torch.arange(len(K) if recipe_i > 0 else k)[not_eor]\n",
    "\n",
    "        if len(K) < 1:\n",
    "            break\n",
    "\n",
    "        ## postcondition: K is the list of hypotheses which have not ended\n",
    "\n",
    "        ## determine inputs for next iter\n",
    "\n",
    "        ## all the same so no need to index using K\n",
    "        if recipe_i == 0:\n",
    "            encoder_houts_i = repeat_for_k(encoder_houts, len(K), dim=0)\n",
    "            ingredients_i = repeat_for_k(ingredients, len(K), dim=0)\n",
    "        else:\n",
    "            encoder_houts_i = encoder_houts_i[:len(K)] # [N=k, L_i, H]\n",
    "            ingredients_i = ingredients_i[:len(K)]\n",
    "\n",
    "        ## different based on k\n",
    "        decoder_hidden_i = decoder_hidden_i[:, k_origin[not_eor]]\n",
    "        decoder_cell_i = decoder_cell_i[:, k_origin[not_eor]]\n",
    "        pos_constraints_i = [pos_constraints_i[i] for i in not_eor.nonzero().flatten()]\n",
    "    else:\n",
    "        for ki in K.tolist():\n",
    "            all_decoder_outs[ki].append(REC_END)\n",
    "\n",
    "    return all_decoder_outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "E, H, V, L_i = 30, 20, 10, 5\n",
    "# decoder = DecoderRNN(E, H, V)\n",
    "decoder = AttnDecoderRNN(E, H, V, padding_val=999).to(DEVICE)\n",
    "decoder_hidden = torch.rand([1, 1, H]).to(DEVICE)\n",
    "decoder_cell = torch.rand([1, 1, H]).to(DEVICE)\n",
    "encoder_houts = torch.rand([1, L_i, H]).to(DEVICE)\n",
    "ingredients = torch.rand([1, L_i]).to(DEVICE)\n",
    "pos_constraints = [[0, 1], [7], [3, 4, 5], [6], [2,4]]\n",
    "neg_constraints = [[2], [4, 3], [9, 4, 3], [8]]\n",
    "k, alpha, beta =3, 6, 2\n",
    "neg_constraint_penalty, likelihood_penalty, low_irr_satisfaction_penalty = 0.5, 0.2, 0.1 # 2 and 3 probably needs to be much lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4, 6, 0, 1, 7, 3], [2, 7, 6, 0, 1, 3], [2, 4, 6, 0, 1, 3]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_neuro_decoder_iter(decoder, decoder_hidden, decoder_cell, encoder_houts, ingredients, 10,\n",
    "                        pos_constraints, neg_constraints, k, alpha, beta, neg_constraint_penalty,\n",
    "                        likelihood_penalty, low_irr_satisfaction_penalty, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=torch.arange(3)[torch.tensor([0,2])][torch.tensor([0,1]).bool()]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=5, hidden_size=10, num_layers=3).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm(torch.rand())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
